{"text": "Subspace Clustering\nRecently, subspace clustering has aroused great interest in researchers in the database com-\nmunity due to the new challenges associated with the high dimensionality of data sets in\nmodern science and technology.\nMany clustering algorithms have been developed to identify clusters in the whole\ndata space; we refer to these clustering algorithms as conventional clustering algorithms.\nUnfortunately, most of these conventional clustering algorithms do not scale well to cluster\nhigh-dimensional data sets in terms of effectiveness and efficiency because of their inherent\nsparsity. In high-dimensional data sets, we encounter several problems. First, the distance\nbetween any two data points becomes almost the same (Beyer et al., 1999), so it is difficult to\ndifferentiate simila", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 0, "start": 0, "end": 800, "page": 1}
{"text": "nce\nbetween any two data points becomes almost the same (Beyer et al., 1999), so it is difficult to\ndifferentiate similar data points from dissimilar ones. Secondly, clusters are embedded in the\nsubspaces of the high-dimensional data space, and different clusters may exist in different\nsubspaces (Agrawal et al., 1998). Because of these problems, almost all conventional\nclustering algorithms fail to work well for high-dimensional data sets.\nOne possible solution is to use dimension reduction techniques such as principal com-\nponent analysis (PCA) and the Karhunen-Loève transformation (Agrawal et al., 1998) or\nfeature selection techniques. In dimension reduction approaches, one first reduces the di-\nmensionality of the original data set by removing less important variables or by transforming", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 1, "start": 680, "end": 1480, "page": 1}
{"text": " one first reduces the di-\nmensionality of the original data set by removing less important variables or by transforming\nthe original data set into a low-dimensional space and then applies conventional clustering\nalgorithms to the new data set. In feature selection approaches, one finds the dimensions\non which data points are correlated. In both dimension reduction and feature selection\napproaches, it is necessary to prune off some variables, which may lead to significant loss\nof information. This can be illustrated by considering a three-dimensional data set that has\nthree clusters: one embedded in the (x,y)-plane, another embedded in the (y,z)-plane,\nand the third embedded in the (z,x)-plane. For such a data set, application of a dimension\nreduction or a feature selection method is unabl", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 2, "start": 1360, "end": 2160, "page": 1}
{"text": "ded in the (z,x)-plane. For such a data set, application of a dimension\nreduction or a feature selection method is unable to recover all the clustering structures, be-\ncause the three clusters are formed in different subspaces. In general, clustering algorithms\nbased on dimension reduction or feature selection techniques generate clusters that may not\nfully reflect the original structure of a given data set.\nThis difficulty that conventional clustering algorithms encounter in dealing with high-\ndimensional data sets motivates the concept of subspace clustering or projected clustering\nT\nable 15.1. List of some subspace clustering algorithms. Num refers to numerical\nand Mix refers to mixed-type.\nAlgorithms Data Type H/P\nCLIQUE (Agrawal et al., 1998) Num Other\nENCLUS (Cheng et al., 1999) Num ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 3, "start": 2040, "end": 2840, "page": 1}
{"text": " refers to mixed-type.\nAlgorithms Data Type H/P\nCLIQUE (Agrawal et al., 1998) Num Other\nENCLUS (Cheng et al., 1999) Num Other\nMAFIA (Goil et al., 1999) Num Other\nPROCLUS (Aggarwal et al., 1999) Num Partitioning\nORCLUS (Aggarwal and Yu, 2000) Num Partitioning\nFINDIT (Woo and Lee, 2002) Num Partitioning\nFLOC (Yang et al., 2002a) Num Partitioning\nDOC (Procopiuc et al., 2002) Num Partitioning\nPART (Cao and Wu, 2002) Num Hierarchical\nCLTree (Liu et al., 2000) Num Other\nCOSA Mix Other\n(Agrawal et al., 1998), whose goal is to find clusters embedded in subspaces of the original\ndata space with their own associated dimensions. In other words, subspace clustering finds\nclusters and their relevant attributes from a data set. Table 15.1 lists a few popular subspace\nclustering algorithms that will be i", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 4, "start": 2720, "end": 3520, "page": 2}
{"text": " their relevant attributes from a data set. Table 15.1 lists a few popular subspace\nclustering algorithms that will be introduced in this chapter.\n15.1 CLIQUE\nCLIQUE (Agrawal et al., 1998) is a clustering algorithm that is able to identify dense\nclusters in subspaces of maximum dimensionality. It is also the first subspace clustering\nalgorithm. This algorithm takes two parameters: ξ, which specifies the number of intervals\nin each dimension, and τ, which is the density threshold. The output is clusters, each of\nwhich is represented by a minimal description in the form of a disjunct normal form (DNF)\nexpression. One disadvantage of this algorithm is that it can only find clusters embedded\nin the same subspace. The clustering model can be described as follows.\nLet A ={ A\n1,A2,...,A d}be a se", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 5, "start": 3400, "end": 4200, "page": 2}
{"text": "lusters embedded\nin the same subspace. The clustering model can be described as follows.\nLet A ={ A\n1,A2,...,A d}be a set of bounded, totally ordered numerical domains\n(for categorical data, each Ai should be a finite set of categorical values). Let S = A1 ×\nA2 ×···× Ad be a d-dimensional numerical space. Let D ={x1,x2,...,x n}be a database,\nwhere xi =(xi1,xi2,...,x id), and the jth component of xi is drawn from Aj.\nThe data space S is partitioned into nonoverlapping rectangular units that are ob-\ntained by partitioning each dimension into ξ (an input parameter) intervals of equal length.\nEach unit u is the intersection of one interval from each dimension, and it has the form\n{u1,u2,...,u d}, where ui =[li,hi)is a right open interval in the partitioning of dimension\nAi. A data point x =(x1", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 6, "start": 4080, "end": 4880, "page": 2}
{"text": "form\n{u1,u2,...,u d}, where ui =[li,hi)is a right open interval in the partitioning of dimension\nAi. A data point x =(x1,x2,...,x d)is said to be contained in a unit u ={u1,u2,...,u d}\nif the following condition is satisfied:\nli ≤xi <h i for all ui.\nA\nunit u is said to be dense if selectivity(u) > τ , where selectivity(u) is the\nselectivity of unit u, which is defined to be the fraction of total data points contained in the\nunit u; the density threshold τ is another input parameter. The units in all subspaces of the\noriginal d-dimensional space can be defined similarly. Let S′={At1 ,At2 ,...,A tr }be any\nsubspace of S, where l<d and ti <t j if i<j . The units in S′are the intersection of an\ninterval from each of the r dimensions of the subspace S′.\nIn the CLIQUE algorithm, a cluster is def", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 7, "start": 4760, "end": 5560, "page": 2}
{"text": " intersection of an\ninterval from each of the r dimensions of the subspace S′.\nIn the CLIQUE algorithm, a cluster is defined to be a maximal set of connected dense\nunits in r dimensions. Two r-dimensional units are said to be connected if they have a\ncommon face or if there exists another r-dimensional unit such that the unit connects the\ntwo units. Two units u1 ={ rt1 ,rt2 ,...,r tr } and u2 ={ r′\nt1 ,r′\nt2 ,...,r ′\ntr } are said to have\na common face if there are l −1 dimensions, assumed to be At1 ,At2 ,...,A tr , such that\nrtj =r′\ntj (1 ≤j ≤l −1) and either htr =l′\ntr or h′\ntr =ltr .\nA region in r dimensions is an axis-parallel rectangular r-dimensional set that can be\nexpressed as unions of units. A regionRis said to be contained in a clusterC if R∩C =R.\nA region R contained in a clust", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 8, "start": 5440, "end": 6240, "page": 3}
{"text": "\nexpressed as unions of units. A regionRis said to be contained in a clusterC if R∩C =R.\nA region R contained in a cluster C is said to be maximal if no proper superset of R is\ncontained in C. A minimal description of a cluster C is a set R of maximal regions such\nthat their union equals C but the union of any proper subset of R does not equal C.\nThe CLIQUE algorithm consists of three steps. In the first step, the subspaces that\ncontain clusters are identified. In the second step, the clusters embedded in the subspaces\nidentified in step 1 are found. Finally, a minimal description of each cluster is generated.\nIn the first step, a bottom-up algorithm is employed to find the dense units. The\nessential observation is that if a set of points S is a cluster in an r-dimensional space, then\nS is", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 9, "start": 6120, "end": 6920, "page": 3}
{"text": "he dense units. The\nessential observation is that if a set of points S is a cluster in an r-dimensional space, then\nS is also part of a cluster in any (r −1)-dimensional projections of this space. Based\non this fact, the algorithm proceeds level by level. It first determines one-dimensional\ndense units, and then continues iteratively: when the (r −1)-dimensional dense units are\ndetermined, the r-dimensional dense units are determined as follows. Let D\nr−1 be the\nset of all (r −1)-dimensional dense units, and let Cr be the set of r-dimensional units\ngenerated from Dr−1 as follows. For two units u1, and u2 in Dr−1,i f u1.aj = u2.aj,\nu1.lj = u2.lj, and u1.hj = u2.hj for j = 1,2,...,r −2, and u1.ar−1 <u 2.ar−1, then\ninsert u =u1.[l1,h1)×u1.[l2,h2)×···× u1.[lr−1,hr−1)×u2.[lr−1,hr−1)into Cl, whe", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 10, "start": 6800, "end": 7600, "page": 3}
{"text": "1,2,...,r −2, and u1.ar−1 <u 2.ar−1, then\ninsert u =u1.[l1,h1)×u1.[l2,h2)×···× u1.[lr−1,hr−1)×u2.[lr−1,hr−1)into Cl, where\nu.ai is the ith dimension of unit u and u.[li,hj) is the interval in the ith dimension of u,\nand the relation < represents lexicographic ordering on attributes. Then Dr is obtained by\ndiscarding those dense units from Cr that have a projection in (r −1) dimensions that is\nnot included in Cr−1.\nIn order to make the first step faster, the minimal description length (MDL) prin-\nciple is applied to decide which subspaces and corresponding dense units are interesting.\nAssume there are S1,S2,...,S m subspaces found in the rth level (then S1,S2,...,S m are\nr-dimensional subspaces). Let c(Sj) be the coverage of subspace Sj defined as\nc(Sj) =\n∑\nu∈Sj\n|u|,\nwhere |u| is the number", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 11, "start": 7480, "end": 8280, "page": 3}
{"text": "-dimensional subspaces). Let c(Sj) be the coverage of subspace Sj defined as\nc(Sj) =\n∑\nu∈Sj\n|u|,\nwhere |u| is the number of points that fall inside u. Subspaces with large coverage are\nselected to form candidate units in the next level of the dense unit generation algorithm, and\nthe rest are pruned. Let the subspaces be sorted in decreasing order of their coverage. The\nsorted list is divided into the selected set I and the pruned set P. Assume c(S\n1)>c ( S2)>\n··· >c\n( Sm). Then I ={ S1,S2,...,S i0 }and P ={ Si0+1,Si0+2,...,S m}for some i.I n\nCLIQUE, i0 is selected such that the total length of encoding CL(i) is minimized, where\nCL(i) is defined as\nCL(i) =log2(μI(i)) +\ni∑\nj=1\nlog2(|c(Sj)−μI(i)|)\n+log2(μP(i)) +\nm∑\nj=i+1\nlog2(|c(Sj)−μP(i)|),\nwhere μI(i) and μP(i) are defined as\nμI(i) =\n\n\n", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 12, "start": 8160, "end": 8960, "page": 3}
{"text": "j=1\nlog2(|c(Sj)−μI(i)|)\n+log2(μP(i)) +\nm∑\nj=i+1\nlog2(|c(Sj)−μP(i)|),\nwhere μI(i) and μP(i) are defined as\nμI(i) =\n\n\n\ni∑\nj=1\nc(Sj)\ni\n\n\n\n,\nμ\nP(i) =\n\n\n\nm∑\nj=i+1\n(Sj)\nn−i\n\n\n\n,\nwhere ⌈·⌉is the ceiling function, i.e., ⌈x⌉is the least integer not less than x.\nThe second step of the CLIQUE algorithm is to find the clusters based on a set of\ndense units C found in the first step. All units inC are in the samer-dimensional space S.I n\nthis step, C will be partitioned intoC\n1,C2,...,C k such that all units inCi are connected and\nunits in different groups are not connected. This process is converted to finding connected\ncomponents in a graph defined as follows: graph vertices correspond to dense units, and\ntwo vertices have an edge between them if and only if the corresponding dense un", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 13, "start": 8840, "end": 9640, "page": 4}
{"text": "vertices correspond to dense units, and\ntwo vertices have an edge between them if and only if the corresponding dense units have\na common face. The depth-first search algorithm (Hopcroft and Tarjan, 1973; Aho et al.,\n1974) is used to find the connected components of the graph.\nThe third step is to generate minimal cluster descriptions for each cluster. The input\nto this step is disjoint sets of connected r-dimensional units in the same subspace. Given a\ncluster C in an r-dimensional subspace S, a set R of regions in the same subspace S is said\nto be a cover of C if every region in R is contained in C and each unit in C is contained in\nat least one of the regions in R. In this step, a minimal cover is found for each cluster by\nfirst greedily covering the cluster by a number of maximal regio", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 14, "start": 9520, "end": 10320, "page": 4}
{"text": "this step, a minimal cover is found for each cluster by\nfirst greedily covering the cluster by a number of maximal regions and then discarding the\nredundant regions.\nThe time complexity in step 1 isO(c\nk+mk)for a constantc. In step 2 and subsequent\nsteps, the dense units are assumed to be stored in memory. The total number of data structure\naccesses is 2kn in step 2.\n15.2 PROCLUS\nPROCLUS (PROjected CLUSting) (Aggarwal et al., 1999) is a variation of the k-medoid\nalgorithm (Kaufman and Rousseeuw, 1990) in subspace clustering. The input parameters\nof the algorithm are the number of clusters k and the average number of dimensions l;\nthe output is a partition {C\n1,C2,...,C k,O} together with a possibly different subset of\ndimensions Pi for each cluster Ci, with O denoting the outlier cluster.\n", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 15, "start": 10200, "end": 11000, "page": 4}
{"text": "O} together with a possibly different subset of\ndimensions Pi for each cluster Ci, with O denoting the outlier cluster.\nThe\nalgorithm consists of three phases: the initialization phase, the iteration phase,\nand the refinement phase. In the initialization phase, a random set of k medoids is chosen\nby applying the greedy technique (Gonzalez, 1985; Ibaraki and Katoh, 1988) to samples\nof the original data set. In the iteration phase, the algorithm progressively improves the\nquality of medoids by iteratively replacing the bad medoids with new ones. The last phase\ncomputes new dimensions associated with each medoid and reassigns the points to the\nmedoids relative to the new sets of dimensions. The PROCLUS algorithm finds out the\nsubspace dimensions of each cluster via a process of evaluating the", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 16, "start": 10880, "end": 11680, "page": 4}
{"text": "s of dimensions. The PROCLUS algorithm finds out the\nsubspace dimensions of each cluster via a process of evaluating the locality of the space\nnear it.\nAlgorithm 15.1. The PROCLUS algorithm.\nRequire: D-Data set, k-Number of clusters, l-Average dimensions of cluster;\n1: Let A,B be constant integers;\n2: Draw a sample S of size A·k randomly;\n3: Let medoids set M ⇐Greedy(S,B ·k);\n4: Let BestObjective ⇐∞ and Mcurrent ⇐random set of medoids{m1,m2,...,m k}⊂\nM;\n5: repeat\n6: Let δi be the distance to the nearest medoid from mi for i =1,2 ...,k ;\n7: Let Li be the set of points in a sphere centered atmi with radiusδi for i =1,2,...,k ;\n8: (P1,P2,...,P k) ⇐FindDimensions(k,l,L 1,L2,...,L k);\n9: (C1,C2,...,C k) ⇐AssignPoints(P 1,P2,...,P k);\n10: Objective ⇐EvaluateClusters(C 1,...,C k,P1,...,P k);\n11: ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 17, "start": 11560, "end": 12360, "page": 5}
{"text": "..,L k);\n9: (C1,C2,...,C k) ⇐AssignPoints(P 1,P2,...,P k);\n10: Objective ⇐EvaluateClusters(C 1,...,C k,P1,...,P k);\n11: if Objective<BestObjective then\n12: Let BestObjective ⇐Objective, Mbest ⇐Mcurrent ;\n13: Compute the bad medoids in Mbest ;\n14: end if\n15: Compute Mcurrent by replacing the bad medoids in Mbest with random points from\nM;\n16: until Stop criterion\n17: (P1,P2,...,P k) ⇐FindDimensions(k,l,L 1,L2,...,L k);\n18: (C1,C2,...,C k) ⇐AssignPoints(P 1,P2,...,P k);\n19: Return Mbest ,P1,P2,...,P k;\nThe pseudocode of the algorithm is described in Algorithm 15.1. In the initialization\nphase, a medoid candidate set M is selected using the greedy technique. In order to reduce\nthe running time of the initialization phase, a sampleS is drawn randomly from the original\ndata set. The procedure o", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 18, "start": 12240, "end": 13040, "page": 5}
{"text": "ce\nthe running time of the initialization phase, a sampleS is drawn randomly from the original\ndata set. The procedure of Greedy(S,t) can be described as follows:\n1: M ⇐{m1}{m1 is a random point of S};\n2: Let dist(x) ⇐d(x,m 1)for each x ∈S\\M;\n3: for i =2t ot do\n4: Let mi ∈S\\M be such that dist(m i) =max{dist(x) :x ∈S\\M};\n5: M ⇐M ∪{mi};\n6: Let dist(x) ⇐min{dist(x),d(x,m i)};\n7: end for\n8: Return M.\nIn the iteration phase, the quality of medoids is improved progressively by iteratively\nreplacing the bad medoids. This phase consists of three major procedures:FindDimensions,\nAssignPoints, and EvaluateClusters . In order to find the subspace dimensions, PRO-\nCLUS evaluates the locality of the space near the medoids. Let Li be the set of points in a\nsphere centered at mi with radius δi, where δi", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 19, "start": 12920, "end": 13720, "page": 5}
{"text": " locality of the space near the medoids. Let Li be the set of points in a\nsphere centered at mi with radius δi, where δi =min1≤j≤k,j̸=id(mi,mj). Let Xij,Yi, and\nσi be defined as\nXij = 1\n|Li|\n∑\nx∈Li\nd(xj,mij),\nYi =\nd∑\nj=1\nXij\nd ,\nσi =\n\n 1\nd −1\nd∑\nj=1\n(Xij −Yi)2\n\n\n1\n2\nfor i =1,2,...,k and j =1,2,...,d , wherexj and mij are the values of thejth dimension\nof x and mi, respectively, and d(·,·)is the Manhattan segmental distance.\nLet Zij = Xij−Yi\nσi\nfor i =1,2,...,k and j =1,2,...,d . The goal of the procedure\nFindDimensions is to pick up the k·l numbers with the least values of Zij subject to the\nconstraint that there are at least two dimensions for each cluster and to put dimension j to\nPi if Zij is picked. This can be achieved by the greedy technique.\nThe AssignPoints procedure assigns ea", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 20, "start": 13600, "end": 14400, "page": 6}
{"text": " dimension j to\nPi if Zij is picked. This can be achieved by the greedy technique.\nThe AssignPoints procedure assigns each pointx in the data set to the clusterCi such\nthat dPi (x,mi) has the lowest values among dP1 (x,m1),...,d Pk(x,mk), where dPi (x,mi)\nis the Manhattan segmental distance fromx to the medoidmi relative to dimensionsPi, i.e.,\ndPi (x,mi) = 1\n|Pi|\n∑\nj∈Pi\n|xj −mij|.\nThe EvaluateClusters procedure evaluates the quality of a set of medoids as the\naverage Manhattan segmental distance from the points to the centroids of the clusters to\nwhich they belong. The quantity that indicates the goodness of a clustering is defined as\n1\nn\nk∑\ni=1\n|Ci|·w i, (15.1)\nwhere nis the number of points in the data set andwi (i =1,2,...,k) are weights defined\nas\nwi = 1\n|Pi|·| Ci|\n∑\nj∈Pi\n∑\nx∈Ci\n|xj −z", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 21, "start": 14280, "end": 15080, "page": 6}
{"text": " the number of points in the data set andwi (i =1,2,...,k) are weights defined\nas\nwi = 1\n|Pi|·| Ci|\n∑\nj∈Pi\n∑\nx∈Ci\n|xj −zij|,\nwhere Pi is the dimension associated with Ci, and zi is the centroid of Ci, i.e.,\nzi = 1\n|Ci|\n∑\nx∈Ci\nx.\nThe\noptimal clustering should have a minimum quantity, which is defined in (15.1).\nThe goal of the iteration phase is to find an approximation of such an optimal clustering.\nAlso in the iteration phase, bad medoids are determined. The medoid of any cluster that has\nless than n\nk ·σmin points is bad, where σmin is a constant smaller than 1. In PROCLUS, σmin\nis set to be 0.1. Bad medoids may be outliers.\nAfter the iteration phase, the best set of medoids is found. The refinement phase\ndoes one more pass over the data to improve the quality of the clustering. Outliers", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 22, "start": 14960, "end": 15760, "page": 6}
{"text": "doids is found. The refinement phase\ndoes one more pass over the data to improve the quality of the clustering. Outliers are also\nhandled in the refinement phase. The running time for computing the segmental distances\nis O(nkl) for each iteration.\n15.3 ORCLUS\nORCLUS (arbitrarily ORiented projected CLUSter generation) (Aggarwal and Yu, 2000) is\nan extension of PROCLUS. It diagonalizes the covariance matrix of each cluster and finds\ninformation about projection subspaces from the diagonalization of the covariance matrix.\nIn order to make the algorithm scale to large databases, ORCLUS also uses extended cluster\nfeature (ECF) vectors (Zhang et al., 1996) and a progressive random sampling approach in\nthe algorithm.\nIn the algorithm, a generalized projected cluster is defined for a pair (C,P), w", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 23, "start": 15640, "end": 16440, "page": 7}
{"text": "dom sampling approach in\nthe algorithm.\nIn the algorithm, a generalized projected cluster is defined for a pair (C,P), where\nC is a set of data points and P is a set of vectors, such that the data points in C are closely\nclustered in the subspace defined by the vectors inP. Unlike CLIQUE (Agrawal et al., 1998),\nthe subspaces for different clusters found by the ORCLUS algorithm may be different.\nThe ORCLUS algorithm takes two input parameters: the number of clusters k and\nthe dimensionality l of the subspace in which each cluster is reported. The output of the\nalgorithm consists of two parts: a (k +1)-way partition {C\n1,C2,...,C k,O}of the data\nset and a possibly different orthogonal set Pi of vectors for each cluster Ci (1 ≤ i ≤ k),\nwhere O is the outlier cluster, which can be assumed to b", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 24, "start": 16320, "end": 17120, "page": 7}
{"text": " orthogonal set Pi of vectors for each cluster Ci (1 ≤ i ≤ k),\nwhere O is the outlier cluster, which can be assumed to be empty. For each cluster Ci, the\ncardinality of the corresponding set Pi is l, which is the user-specified parameter.\nAlgorithm 15.2. The pseudocode of the ORCLUS algorithm.\nRequire: D-Data set, k-Number of clusters, and l-Number of dimensions;\n1: Pick k0 >k initial data points from D and denote them by S ={s1,s2,...,s k0 };\n2: Set kc ⇐k0 and lc ⇐d;\n3: For each i, set Pi to be the original axis system;\n4: Set α ⇐0.5 and compute β according to equation (15.3);\n5: while kc >k do\n6: (s1,...,s kc,C1,...,C kc) =Assign(s1,...,s kc,P1,...,P kc) {find the partitioning\ninduced by the seeds};\n7: Set knew ⇐max{k,kc ·α}and lnew ⇐max{l,lc ·β};\n8: (s1,...,s knew,C1,...,C knew,P1,...,P", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 25, "start": 17000, "end": 17800, "page": 7}
{"text": "tioning\ninduced by the seeds};\n7: Set knew ⇐max{k,kc ·α}and lnew ⇐max{l,lc ·β};\n8: (s1,...,s knew,C1,...,C knew,P1,...,P knew) =Merge(C1,...,C kc,k new,lnew);\n9: Set kc ⇐knew and lc ⇐lnew;\n10: end while\n11: for i =1t ok do\n12: Pi =FindVectors(C i,l);\n13: end for\n14: (s1,...,s k,C1,...,C k) =Assign(s1,...,s k,P1,...,P k);\n15: Output (C1,...,C k);\nThe ORCLUS algorithm consists of a number of iterations, in each of which a variant\nof the hierarchical merging method is applied in order to reduce the number of current\nclusters by the factor α< 1. Initially, the dimensionality of each cluster is equal to the full\ndimensionality d. In each iteration, the dimensionality of the current clusters is reduced by\nthe factor β< 1. Finally, the dimensionality of the clusters will be reduced gradually to t", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 26, "start": 17680, "end": 18480, "page": 7}
{"text": "rrent clusters is reduced by\nthe factor β< 1. Finally, the dimensionality of the clusters will be reduced gradually to the\nuser-specified dimensionality l. In order to make the algorithm fast, a small number k0 of\ninitial points are picked as seeds. At each stage of the algorithm, there is a projected cluster\n(C\ni,Pi) associated with each seed si. At each stage of the algorithm, kc and lc denote the\nnumber of current clusters and the dimensionality of the current clusters, respectively.\nIn order to make the reduction from k0 to k clusters occur in the same number of\niterations as the reduction from l0 =d to l dimensions, the values of α and β must satisfy\nthe relationship\nk0αN =k and l0βN =l,\nwhere N is the number of iterations, which is equivalent to\nlog 1\nα\n(k0\nk\n)\n=log 1\nβ\n(d\nl\n)\n. (15.", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 27, "start": 18360, "end": 19160, "page": 8}
{"text": "\nk0αN =k and l0βN =l,\nwhere N is the number of iterations, which is equivalent to\nlog 1\nα\n(k0\nk\n)\n=log 1\nβ\n(d\nl\n)\n. (15.2)\nIn the ORCLUS algorithm, the value of α is set to 0.5 and the value of β is calculated\naccording to equation (15.2), i.e.,\nβ =− exp\n(\n(ln d\nl )·(ln 1\nα)\nln k0\nk\n)\n. (15.3)\nThe ORCLUS algorithm is briefly described inAlgorithm 15.2. In the algorithm, there\nare three subprocedures: Assign(s1,...,s kc,P1,...,P kc), Merge(s1,...,s kc,knew,lnew),\nand FindVectors(C i,q). Like most partitional clustering algorithms, the Assign proce-\ndure assigns each data point to its nearest seed according to the projected distance. Letx and\ny be two data points in the original d-dimensional space, and let P ={ e1,e2,...,e p}be\na set of orthonormal vectors. Then the projected distance betwe", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 28, "start": 19040, "end": 19840, "page": 8}
{"text": "ginal d-dimensional space, and let P ={ e1,e2,...,e p}be\na set of orthonormal vectors. Then the projected distance between x and y in the subspace\ndefined by the vectors inP is defined to be the Euclidean distance between their projections\nin the p-dimensional space represented by P, i.e.,\ndP(x,y,P) =\n( p∑\ni=1\n(x ·ei −y ·ei)2\n)1\n2\n, (15.4)\nwhere x ·ei and y ·ei denote the dot-products of x and ei and y and ei, respectively.\nAlgorithm 15.3. Assign(s1,...,s kc ,P 1,...,P kc ).\nRequire: D-Data set;\n1: Set Ci ⇐pνhiEfor i =1,2,...,k c;\n2: for all data points x ∈D do\n3: Let dmin ⇐dP(x,s1,P1)and imin ⇐1;\n4: for i =2t okc do\n5: if dmin >d P(x,si,Pi)then\n6: dmin ⇐dP(x,si,Pi);\n7: imin ⇐i;\n8: end if\n9: end for\n10: Assign x to Cimin ;\n11: end for\n12: Set si ⇐  ̄X(Ci)for i =1,2,...,k c {  ̄X(Ci) is the", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 29, "start": 19720, "end": 20520, "page": 8}
{"text": "min ⇐i;\n8: end if\n9: end for\n10: Assign x to Cimin ;\n11: end for\n12: Set si ⇐  ̄X(Ci)for i =1,2,...,k c {  ̄X(Ci) is the centroid of cluster Ci};\n13: Return (s1,...,s kc,C1,...,C kc);\nThe Assign procedure is described in Algorithm 15.3. In this procedure, the vector\nsets Pi (i = 1,2,...,k c) are fixed, and each data point is assigned to the nearest cluster\naccording to the projected distance defined in (15.4). Inside the Assign procedure,  ̄X(C)\ndenotes the centroid of C, which is defined as\n ̄X(C) =\n∑\nx∈C\nx\n|C|. (15.5)\nAlgorithm 15.4. Merge(C1,...,C kc ,K new,l new).\n1: if knew ≤k then\n2: Exit;\n3: end if\n4: for i =1t okc do\n5: for j =i +1t ok c do\n6: Pij =FindVectors(C i ∪Cj,lnew);\n7: sij =  ̄X(Ci ∪Cj){centroid of Ci ∪Cj};\n8: rij =R(Ci ∪Cj,Pij);\n9: end for\n10: end for\n11: while kc >k new ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 30, "start": 20400, "end": 21200, "page": 9}
{"text": " ∪Cj,lnew);\n7: sij =  ̄X(Ci ∪Cj){centroid of Ci ∪Cj};\n8: rij =R(Ci ∪Cj,Pij);\n9: end for\n10: end for\n11: while kc >k new do\n12: Find i′,j ′such that ri′j′ = min\n1≤i<j≤kc\nrij;\n13: Set si′ ⇐si′j′, Ci′ ⇐Ci′∪Cj′, Pi′ ⇐Pi′j′{merge Ci′ and Cj′};\n14: Discard seed sj′ and Cj′, renumber the seeds and clusters indexed larger than j′by\nsubtracting 1, and renumber sij,Pij,rij correspondingly for any i,j ≥j′;\n15: for j =1,j ̸=i′to kc −1 do\n16: Update Pi′j,si′j and ri′j if i′<j ; otherwise update Pji′,sji′, and rji′;\n17: end for\n18: kc ⇐kc −1;\n19: end while\n20: Return (s1,...,s knew,C1,...,C knew,P1,...,P knew);\nThe Merge procedure is used to reduce the number of clusters from kc to knew =\nα ·kc during a given iteration. The pseudocode of the Merge procedure is described in\nAlgorithm\n15.4. In this proced", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 31, "start": 21080, "end": 21880, "page": 9}
{"text": "w =\nα ·kc during a given iteration. The pseudocode of the Merge procedure is described in\nAlgorithm\n15.4. In this procedure, the two closest pairs of current clusters are merged\nsuccessively. The closeness between two clusters is measured by projected energy. Let\n(C,P) be a projected cluster. Then the projected energy of cluster C in subspace P is\ndefined as\nR(C,P) =\n∑\nx∈C\n1\n|C|d2\nP(x,  ̄X(C),P), (15.6)\nwhere dP(·,·,·) is the projected distance defined in (15.4), and  ̄X(C) is the centroid of C,\nwhich is defined in (15.5).\nAlgorithm 15.5. FindVectors(C, q).\n1: Compute the d ×d covariance matrix M for C;\n2: Find the eigenvectors and eigenvalues of matrix M;\n3: Let P be the set of eigenvectors corresponding to the smallest q eigenvalues;\n4: Return P;\nThe FindVectors procedure (Algorithm 15.5", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 32, "start": 21760, "end": 22560, "page": 9}
{"text": " set of eigenvectors corresponding to the smallest q eigenvalues;\n4: Return P;\nThe FindVectors procedure (Algorithm 15.5) is used to determine the optimal sub-\nspace associated with each cluster given the dimensionality. This is achieved by finding the\neigenvalues and eigenvectors of the covariance matrix for the cluster. For a given cluster\nC, the subspace associated withC is defined by the orthonormal eigenvectors with the least\nspread (eigenvalues).\nThe ORCLUS algorithm can also handle outliers by measuring the projected distance\nbetween each data point and the seed of the cluster to which it belongs. More specifically,\nfor each i ∈{1,...,k\nc}, let δi be the projected distance between the seed si and its nearest\nother seed in subspace Pi, let x be any data point, and let sr be the seed ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 33, "start": 22440, "end": 23240, "page": 10}
{"text": "distance between the seed si and its nearest\nother seed in subspace Pi, let x be any data point, and let sr be the seed to which x is\nassigned. If the projected distance between x and sr in subspace Pr is larger than δr, then x\nis treated as an outlier.\nIn order to make the algorithm scalable to very large databases, the concept of ECF-\nvectors is used, as in BIRCH (Zhang et al., 1996). For each cluster, there is an ECF-vector\nassociated with it. The ECF-vector for each cluster in the ORCLUS algorithm consists of\nd\n2 +d +1 entries. Let C be a cluster. Then the ECF-vector v(C) =(v1,v2,...,v d2+d+1)\nfor C is defined as\nv(i−1)d+j =\n∑\nx∈C\nxi ·xj,i , j =1,2,...,d,\nvd2+i =\n∑\nx∈C\nxi,i =1,2,...,d,\nvd2+d+1 =|C|,\nwhere xi and xj denote the ith and jth components of the data point x, respectively.\nFr", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 34, "start": 23120, "end": 23920, "page": 10}
{"text": "C\nxi,i =1,2,...,d,\nvd2+d+1 =|C|,\nwhere xi and xj denote the ith and jth components of the data point x, respectively.\nFrom the definition of the ECF-vectors, one can see that an ECF-vector can be divided\ninto three parts. The ECF-vectors defined above have some good properties. For example,\nthe ECF-vector satisfies the additive property, i.e., the ECF-vector for the union of two\nclusters is equal to the sum of the corresponding ECF-vectors. Also, the covariance matrix\nfor a cluster can be derived directly from the ECF-vector of the cluster. Namely, let C be a\ncluster\nand v =v(C). Then the covariance matrix M =(mij)d×d for C is\nmij = v(i−1)d+j\nxd2+d+1\n− vd2+i ·vd2+j\nv2\nd2+d+1\n.\nThe running time for the Merge procedure is O(k3\n0 +k2\n0 d), and the running time for\nthe Assign procedure is O( d", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 35, "start": 23800, "end": 24600, "page": 10}
{"text": "d+1\n.\nThe running time for the Merge procedure is O(k3\n0 +k2\n0 d), and the running time for\nthe Assign procedure is O( d\n1−αk0n). Since the running time for subspace determinations\nis strictly dominated by the subspace determinations during the Merge phase, this running\ntime can be ignored. The total running time of the ORCLUS algorithm is thereforeO(k3\n0 +\nk0nd +k2\n0 d).\n15.4 ENCLUS\nENCLUS (ENtropy-based CLUStering) (Cheng et al., 1999) is an entropy-based subspace\nclustering algorithm for clustering numerical data. It can find arbitrarily shaped clusters\nembedded in the subspaces of the original data space. It follows similar approaches sug-\ngested by CLIQUE, but does not make any assumptions about the cluster shapes and hence\nis capable of finding arbitrarily shaped clusters embedded in", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 36, "start": 24480, "end": 25280, "page": 11}
{"text": "ot make any assumptions about the cluster shapes and hence\nis capable of finding arbitrarily shaped clusters embedded in subspaces. The ENCLUS\nalgorithm searches for subspace clusters based on the fact that a subspace with clusters\ntypically has lower entropy than a subspace without clusters. In ENCLUS, the entropy of\na data set is defined as\nH(X) =−\n∑\nx∈χ\nd(x) log d(x), (15.7)\nwhere Xis a set of variables,χ is the set of all units in the space formed byX, andd(x) is the\ndensity of a unit x in terms of the percentage of data points contained in unit x. ENCLUS\nmakes use of the relationship between an entropy and a clustering criterion to identify\nthe subspaces with good clustering, and then applies methods developed in CLIQUE or\nother algorithms to identify all clusters in the subspaces ide", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 37, "start": 25160, "end": 25960, "page": 11}
{"text": "ustering, and then applies methods developed in CLIQUE or\nother algorithms to identify all clusters in the subspaces identified. Entropy and clustering\ncriteria are related. For example, the entropy decreases as the density of the dense units\nincreases, and we also have the following correspondence between entropy and dimension\ncorrelations:\nH(V\n1,V2,...,V i) =\ni∑\nj=1\nH(Vj) (15.8)\nif and only if V1,V2,...,V i are independent;\nH(V1,...,V i,Y) =H(V1,V2,...,V i)\nif and only if Y is a function of V1,V2,...,V i.\nTo discover the subspaces with good clusters, the algorithm uses the fact of downward\nclosure, i.e., if anl-dimensional subspaceV1,V2,...,V l has good clustering, so do all(l−1)-\ndimensional projections of this space. To discover the correlations between variables, the\nalgorithm uses th", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 38, "start": 25840, "end": 26640, "page": 11}
{"text": "o all(l−1)-\ndimensional projections of this space. To discover the correlations between variables, the\nalgorithm uses the fact of upward closure, i.e., if a set of dimensions S is correlated, so is\nevery superset of S. Upward closure allows us to find the minimally correlated subspaces.\nA\nset of variables V1,V2,...,V i are correlated if equation (15.8) is not satisfied.\nDefine the interest of a set of variables by\ninterest( {V1,V2,...,V i}) =\ni∑\nj=1\nH(Vj)−H(V1,V2,...,V i). (15.9)\nLet Sl be the set of l-dimensional significant subspaces and NSl be the set of l-dimensional\nsubspaces with good clustering but not minimal correlation. The procedure for mining\nsignificant subspaces is described in Algorithm 15.6. In this procedure, one first builds\ngrids in the subspace c and calculates the dens", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 39, "start": 26520, "end": 27320, "page": 11}
{"text": "aces is described in Algorithm 15.6. In this procedure, one first builds\ngrids in the subspace c and calculates the density of each cell. Then the entropy for this\nsubspace is computed based on the density information using the formula in (15.7). The\ncandidate set C\nl+1 is generated as follows: select p,q ∈NSl that satisfy p.dimi =q.dimi\nfor i =1,2,...,l −1 and p.diml <q. diml and then insert p and q.diml into Cl+1.\nAlgorithm 15.6. ENCLUS procedure for mining significant subspaces.\nRequire: D-Data set, ω-Entropy threshold, ε-Interest threshold;\n1: Let l ⇐1 and C1 be one-dimensional subspaces;\n2: while Cl ̸=pνhiEdo\n3: for all c ∈Cl do\n4: Calculate the density fc(·);\n5: Calculate the entropy H(c) from fc(·);\n6: if H( c )<ωthen\n7: if interest(c)>ε then\n8: Sl ⇐Sl ∪c;\n9: else\n10: NSl ⇐NSl ∪c;\n1", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 40, "start": 27200, "end": 28000, "page": 12}
{"text": "culate the entropy H(c) from fc(·);\n6: if H( c )<ωthen\n7: if interest(c)>ε then\n8: Sl ⇐Sl ∪c;\n9: else\n10: NSl ⇐NSl ∪c;\n11: end if\n12: end if\n13: end for\n14: Generate Cl+1 from NSl;\n15: l ⇐l +1;\n16: end while\n17: Output ⋃\nl Sl as significant subspaces.\nSince the mining of high-dimensional clusters is often avoided by the above proce-\ndure, another procedure used to mine interesting subspaces has been proposed. Define the\ninterest _gain of a set of variables by\ninterest _gain({V\n1,V2,...,V i})\n=interest( {V1,V2,...,V i})−max\n1≤l≤i\n{interest( {V1,V2,...,V i}\\{Vl})},\nwhere interest( ·) is defined in (15.9). The procedure for mining interesting subspaces is\nsimilar to the one for mining significant subspaces. The modified procedure for mining\ninteresting subspaces is described in Algorithm 15.7", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 41, "start": 27880, "end": 28680, "page": 12}
{"text": "for mining significant subspaces. The modified procedure for mining\ninteresting subspaces is described in Algorithm 15.7. The procedure for mining interesting\nsubspaces\nhas more candidates and a longer running time than the one for mining significant\nsubspaces.\nAlgorithm 15.7. ENCLUS procedure for mining interesting subspaces.\nRequire: D-Data set, ω-Entropy threshold, ε′-Interest threshold;\n1: Let l ⇐1 and C1 be one-dimensional subspaces;\n2: while Cl ̸=pνhiEdo\n3: for all c ∈Cl do\n4: Calculate the density fc(·);\n5: Calculate the entropy H(c) from fc(·);\n6: if H( c )<ωthen\n7: if interest _gain(c) > ε′then\n8: Il ⇐Il ∪c;\n9: else\n10: NIl ⇐NIl ∪c;\n11: end if\n12: end if\n13: end for\n14: Generate Cl+1 from Il ∪NIl;\n15: l ⇐l +1;\n16: end while\n17: Output ⋃\nl Sl as interesting subspaces.\n15.5 FINDIT\nF", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 42, "start": 28560, "end": 29360, "page": 12}
{"text": "for\n14: Generate Cl+1 from Il ∪NIl;\n15: l ⇐l +1;\n16: end while\n17: Output ⋃\nl Sl as interesting subspaces.\n15.5 FINDIT\nFINDIT (a Fast and INtelligent subspace clustering algorithm using DImension voTing) (Woo\nand Lee, 2002) is a subspace clustering algorithm that uses a dimension-oriented distance\nmeasure and a dimension voting policy to determine the correlated dimensions for each\ncluster.\nThe dimension-oriented distance dod in FINDIT is defined as\ndod\nε(x,y) =max{dodε(x →y),dod ε(y →x)}, (15.10)\nwhere dodε(x →y)is the directed dod defined as\ndodε(x →y) =|Qx|−|{ j :|xj −yj|≤ε,j ∈Qx ∩Qy}|, (15.11)\nwhere Qx is the subspace dimension in which the dimensional values of the data point x\nare meaningful.\nThe FINDIT algorithm consists of three phases: the sampling phase, the cluster-\nforming phas", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 43, "start": 29240, "end": 30040, "page": 13}
{"text": "ata point x\nare meaningful.\nThe FINDIT algorithm consists of three phases: the sampling phase, the cluster-\nforming phase, and the data-assigning phase. In the sampling phase, two samples S and\nM are made for the original data set by Chernoff bounds (Motwani and Raghavan, 1995;\nGuha et al., 1998) according to the data set size and a parameter Cminsize . Let Cb(k,n) be\nthe minimum size of sample S such that every cluster has more than ξ data points in the\nsample\nwith probability 1 −δ. Then the Cb(k,n) can be computed from the formula (Woo\nand Lee, 2002; Guha et al., 1998)\nCb(k,n) =ξkρ +kρlog\n(1\nδ\n)\n+kρ\n√(\n2ξ +log 1\nδ\n)\nlog 1\nδ, (15.12)\nwhere k is the number of clusters, ρ is a value satisfying the equation\nρ = n\nk ·|Cmin|,\nand Cmin is the smallest cluster in the partition. FINDIT takes ξ = ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 44, "start": 29920, "end": 30720, "page": 13}
{"text": "is a value satisfying the equation\nρ = n\nk ·|Cmin|,\nand Cmin is the smallest cluster in the partition. FINDIT takes ξ = 30, δ = 0.01, and\nCmin =Cminsize in (15.12) for the sample S, and ξ =1, δ =0.01, and Cmin =Cminsize for\nthe sample M.\nIn the cluster-forming phase, the subspace dimensions of each cluster are determined\nby a method called dimension voting and the best medoid cluster set is formed in order to be\nused in the following data assignment phase. In order to find an appropriate ε, the cluster-\nforming phase is iterated several times with increasing εvalue in [ 1\n100 R, 25\n100 R], where R is\nthe normalized value range. For a fixedε, the V nearest neighbors are sequentially searched\nfrom S for each medoid inM in order to determine the correlated dimensions of the medoid.\nThe distan", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 45, "start": 30600, "end": 31400, "page": 14}
{"text": "quentially searched\nfrom S for each medoid inM in order to determine the correlated dimensions of the medoid.\nThe distance between a point x ∈S and a medoid z ∈M is measured as\ndodε(z,x) =d −|{j :|zj −yj|≤ε,j ∈Q}|,\nwhere Q ={1,2,...,d }and d is the number of dimensions, since no correlated dimensions\nhave been selected for both points yet, i.e., Qx = Qz = Q. After selecting the V nearest\nneighbors, the algorithm calculates the number of selected neighbors that vote “yes” for\neach dimension, i.e., the number of selected neighbors from z is less than or equal to εfor\neach dimension, i.e.,\nV\nj =|{j :|xj −zj|≤ε,j ∈Q}|,j =1,2,...,d.\nIf Vj ≥Vs, then a certain correlation exists in the jth dimension, where Vs is the decision\nthreshold. For example, if V =20, then we could take Vs =12.\nAlgorithm 1", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 46, "start": 31280, "end": 32080, "page": 14}
{"text": " in the jth dimension, where Vs is the decision\nthreshold. For example, if V =20, then we could take Vs =12.\nAlgorithm 15.8. The FINDIT algorithm.\nRequire: D-The Data set, Cminsize -Minimum size of clusters, Dmindist -Minimum differ-\nence between two resultant clusters;\n1: Draw two samples S and M from the data set according to the Chernoff bounds {sam-\npling phase};\n2: Let εi ⇐ i\n100 R for i =1,2,...,25;\n3: Let MCi be the set of medoid clusters for εi;\n4: Set the optimal ε⇐0, MC ⇐pνhiE;\n5: for i =1t o2 5do\n6: Determine correlated dimensions for every medoid in M;\n7: Assign every point in S to the nearest medoid in M;\n8: Merge similar medoids in M to make medoid cluster set MCi;\n9: Refine medoid clusters in MCi {cluster-forming phase};\n10: end for\n11: Let i0 be the subscript such that Soun", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 47, "start": 31960, "end": 32760, "page": 14}
{"text": "et MCi;\n9: Refine medoid clusters in MCi {cluster-forming phase};\n10: end for\n11: Let i0 be the subscript such that Soundness(MC i0 ) = max1≤i≤25 Soundness(MC i),\nand then set ε⇐εi0 , MC ⇐MCi0 ;\n12: Set min_dod ⇐d, nearest _mc ⇐pνhiE;\n13: for all x ∈D do\n14: for all A ∈MCi0 do\n15: for all z ∈A do\n16: if dodεi0\n(z →x) =0 and dodεi0\n(z,x)<m in_dod then\n17: min_dod ⇐dodεi0\n(z,x), nearest _mc ⇐A;\n18: end if\n19: end for\n20: end for\n21: if nearest _mc ̸=pνhiEthen\n22: Assign x to nearest _mc;\n23: else\n24: Assign x to outlier cluster;\n25: end if\n26: end for\nA point x in S is assigned to the nearest z in M satisfying\ndodε(z →x) =|Qz|−|{ j :|zj −xj|≤ε,j ∈Q}|= 0,\nwhere Qz is the set of correlated dimensions of z. If there is more than one such medoid,\nthe point x will be assigned to the medoid which ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 48, "start": 32640, "end": 33440, "page": 15}
{"text": " of correlated dimensions of z. If there is more than one such medoid,\nthe point x will be assigned to the medoid which has the largest number of correlated\ndimensions.\nIn FINDIT, a hierarchical clustering algorithm is employed to cluster the medoids.\nThe distance between two medoid clusters A and B is defined to be the weighted average\nbetween the medoids belonging to them, i.e.,\ndodε(A,B) =\n∑\nzi∈A,zj∈B\n|zi|·| zj|·dod ε(zi,zj)\n(\n∑\nzi∈A\n|zi|\n)\n·\n(\n∑\nzj∈B\n|zj|\n) ,\nwhere |zi|is the number of points assigned to zi, |zj|is defined similarly, dodε(zi,zj) is\ndefined as\ndodε(zi,zj) =max{|Qzi |,|Qzj |}−|{ l :|zil −zjl|≤ε,l ∈Qzi ∩Qzj }|,\nand zil and zjl are the values of the lth dimension of zi and zj, respectively.\nTo evaluate a medoid clustering, a criterion called soundness is defined for eachMC", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 49, "start": 33320, "end": 34120, "page": 15}
{"text": "imension of zi and zj, respectively.\nTo evaluate a medoid clustering, a criterion called soundness is defined for eachMCi,\nthe set of medoid clusters for εi, by the formula\nSoundness(MC i) =\n∑\nA∈MCi\n|A|·| KDA|,\nwhere KDA is the set of key dimensions or correlated dimensions of the medoid clusterA,\nwhich is defined by\nKDA =\n\n\nj :\n∑\nz∈A\nδj(z)·|z|\n∑\nz∈A\n|z| >δ 0\n\n\n,\nwhere δ0 is a threshold that could be taken from 0.9t o1 ,|z|is the number of points assigned\nto the medoid z, and δj(z) =1i fj is a correlated dimension of z; otherwise it is 0.\nIn the data-assigning phase, the data points in the original data set are assigned to the\nnearest medoid cluster in the optimal medoid clustering found in the cluster-forming phase\nor put into the outlier cluster. The FINDIT algorithm is sketched in", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 50, "start": 34000, "end": 34800, "page": 15}
{"text": "edoid clustering found in the cluster-forming phase\nor put into the outlier cluster. The FINDIT algorithm is sketched in Algorithm 15.8.\n15.6 MAFIA\nMAFIA (Merging of Adaptive Finite Intervals) (Nagesh et al., 2000; Goil et al., 1999) is\na parallel subspace clustering algorithm using adaptive computation of the finite intervals\nin each dimension that are merged to explore clusters embedded in subspaces of a high-\ndimensional data set. It is also a density- and grid-based clustering algorithm.\nThe MAFIA algorithm uses adaptive grids to partition the data space into small units\nand then merges the dense units to form clusters. To compute the adaptive grids, it first\nconstructs a histogram by partitioning the data space into a number of nonoverlapping\nregions and mapping the data points to eac", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 51, "start": 34680, "end": 35480, "page": 16}
{"text": "ts a histogram by partitioning the data space into a number of nonoverlapping\nregions and mapping the data points to each cell. The procedure of computing adaptive\ngrids is listed in Algorithm 15.9.\nAlgorithm 15.9. Procedure of adaptive grids computation in MAFIA the algorithm.\nRequire: D\ni – Domain of ith attribute; N – Total number of data points in the data set; a\n– Size of the generic interval;\n1: for i =1t od do\n2: Divide Di into intervals of some small size x;\n3: Compute the histogram for each interval in the ith dimension, and set the value of\nthe window to the maximum in the window;\n4: From left to right, merge two adjacent intervals if they are within a threshold β;\n5: if number of bins == 1 then\n6: Divide the ith dimension into a fixed number of equal intervals and set a threshol", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 52, "start": 35360, "end": 36160, "page": 16}
{"text": " β;\n5: if number of bins == 1 then\n6: Divide the ith dimension into a fixed number of equal intervals and set a threshold\nβ′for it;\n7: end if\n8: end for\nMAFIA is a parallel clustering algorithm. In its implementation, MAFIA introduces\nboth data parallelism and task parallelism. The main properties of this algorithm are listed\nin Table 15.2.\nT\nable 15.2. Description of the MAFIA algorithm, where c is a constant, k is the\nhighest dimensionality of any dense cells in the data set, p is the number of processors, N\nis the number of data points in the data set,B is the number of records in the memory buffer\non each processor, andγ is the I/O access time for a block ofB records from the local disk.\nClustering Data Type Numerical data\nCluster Shape Centered cluster\nTime Complexity O(ck + N\npB kγ +", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 53, "start": 36040, "end": 36840, "page": 16}
{"text": "from the local disk.\nClustering Data Type Numerical data\nCluster Shape Centered cluster\nTime Complexity O(ck + N\npB kγ +αSpk)\nAlgorithm Type Parallel, hierarchical\n15.7 DOC\nDOC (Density-based Optimal projective Clustering) (Procopiuc et al., 2002) is a Monte\nCarlo–based algorithm that computes a good approximation of an optimal projective cluster.\nThe DOC algorithm can identify arbitrarily shaped clusters from the data sets, for example,\nimage data.\nIn the DOC algorithm, a projective cluster of width ω is defined as a pair (C,P),\nwhere C is a subset of data points of the original data set, and P is a subset of dimensions\nassociated with C such that\n1. C is α-dense, i.e., |C|≥α |S|, where α ∈[0,1]and S is the original data set;\n2. ∀j ∈P, maxx∈C xj −miny∈C yj ≤ω, where ω ≥0;\n3. ∀j ∈Pc ={1,2,", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 54, "start": 36720, "end": 37520, "page": 17}
{"text": "|C|≥α |S|, where α ∈[0,1]and S is the original data set;\n2. ∀j ∈P, maxx∈C xj −miny∈C yj ≤ω, where ω ≥0;\n3. ∀j ∈Pc ={1,2,...,d }\\P, maxx∈C xj −miny∈C yj >ω , where ω ≥0.\nLet μ : R ×R → R be a function such that μ(0,0) = 0 and μ is monotonically\nincreasing in each argument. Then the quality of a projective cluster (C,P) is defined to\nbe μ(|C|,|P|). For any 0 ≤ β< 1, the function μ is said to be β-balanced if μ(x,y) =\nμ(βx,y +1) for all x> 0,y ≥0. One choice of such a function is μ(x,y) =x(1\nβ)y(0 <\nβ< 1).\nA projective cluster (C,P) is said to be μ-optimal if it maximizes μ over Pα, where\nPα is the set of all α-dense projective clusters of width at most ω from D. Monotonicity\nis required since we want the projective cluster (C,P) so that C and P are maximal. The\nβ-balanced condition is used t", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 55, "start": 37400, "end": 38200, "page": 17}
{"text": "y\nis required since we want the projective cluster (C,P) so that C and P are maximal. The\nβ-balanced condition is used to specify the tradeoff between the number of data points and\nthe number of dimensions in a cluster.\nAlgorithm 15.10.The DOC algorithm for approximating an optimal projective clus-\nter.\nRequire: D-Data set; α, β, and ω;\n1: Let r ⇐ log(2d)\n−log(2β), m ⇐\n(2\nα\n)r\nln 4;\n2: for i =1t o 2\nα do\n3: Choose x ∈D uniformly at random;\n4: for j =1t om do\n5: Choose X ⊆D of size r uniformly at random;\n6: P ⇐{j :|yj −xj|≤ω, ∀y ∈X};\n7: C ⇐D ∩Bx,P;\n8: if |C|<α |D|then\n9: (C,P) ⇐(pνhiE,pνhiE);\n10: end if\n11: end for\n12: end for\n13: Return cluster (C0,P0)that maximizes μ(|C|,|P|)over all computed clusters (C,P).\nLet (C,P) be a projective cluster. An intuitive geometric objective is an axis-\np", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 56, "start": 38080, "end": 38880, "page": 17}
{"text": ",|P|)over all computed clusters (C,P).\nLet (C,P) be a projective cluster. An intuitive geometric objective is an axis-\nparalleled box given as\nBC,P =[l1,h1]×[ l2,h2]×···×[ ld,hd],\nwhere lj and hj (j =1,2,...,d) are defined as\nlj =\n{\n−∞ if j ̸∈P,\nmin\nx∈C\nxj if j ∈P,\nhj =\n{\n∞ if j ̸∈P,\nmax\nx∈C\nxj if j ∈P.\nFrom the definition of BC,P , a point x ∈C if and only if x is contained in BC,P . Let x be a\ndata point and let Bx,P be defined as\nBx,P =[l1,h1]×[ l2,h2]×···×[ ld,hd],\nwhere lj and hj(j =1,2,...,d) are defined as\nlj =\n{ −∞ if j ̸∈P,\nxj −ω if j ∈P,\nhj =\n{ ∞ if j ̸∈P,\nxj +ω if j ∈P.\nThen for any x ∈C, we have BC,P ⊆Bx,P. The Monte Carlo algorithm for approximating\na projective cluster is described in Algorithm 15.10. It has been shown that the Monte Carlo\nalgorithm works correctly (Procopiuc", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 57, "start": 38760, "end": 39560, "page": 18}
{"text": "ive cluster is described in Algorithm 15.10. It has been shown that the Monte Carlo\nalgorithm works correctly (Procopiuc et al., 2002). The total running time of the Monte\nCarlo algorithm described above is O(nd\nc), where c = log α−log 2\nlog(2β) .\nThere are three parameters in the DOC algorithm, i.e., α,β, and ω. In most experi-\nments reported in (Procopiuc et al., 2002), these parameters are set as α =0.1, β =0.25,\nand ω =15. Since the parameter ω controls the width of the box, it is hard to choose the\nright value for ω if we do not have much information about the data set. In order to choose\na good value for ω, a heuristic method has been proposed in (Procopiuc et al., 2002) as\nfollows. For a data point xi, let yi be its nearest neighbor. Then let ω be defined as\nω =r\nn∑\ni=1\nwi\nn ,\nwhere", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 58, "start": 39440, "end": 40240, "page": 18}
{"text": "2002) as\nfollows. For a data point xi, let yi be its nearest neighbor. Then let ω be defined as\nω =r\nn∑\ni=1\nwi\nn ,\nwhere r is\na small constant, and wi (i =1,2,...,n) are given by\nwi =\nd∑\nj=1\n|xij −yij|\nd .\n15.8 CLTree\nCLTree (CLustering based on decision Trees) (Liu et al., 2000) is a clustering algorithm for\nnumerical data based on a supervised learning technique called decision tree construction.\nThe CLTree algorithm is able to find clusters in the full dimension space as well as in\nsubspaces. The resulting clusters found by CLTree are described in terms of hyperrectangle\nregions. The CLTree algorithm is able to separate outliers from real clusters effectively,\nsince it naturally identifies sparse and dense regions.\nThe basic idea behind CLTree is to regard each data point in the origina", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 59, "start": 40120, "end": 40920, "page": 18}
{"text": " naturally identifies sparse and dense regions.\nThe basic idea behind CLTree is to regard each data point in the original data set as\nhaving a class Y and to assume that the data space is uniformly distributed with another\ntype of points given class N. The main goal of CLTree is to partition the data space into\ndata (dense) regions and empty (sparse) regions. The CLTree algorithm consists of two\nsteps. The first step is to construct a cluster tree using a modified decision tree algorithm;\nthe second step is to prune the cluster tree interactively. The final clusters are described as\na list of hyperrectangle regions.\nDecision tree construction is a technique for classification. A decision tree has two\ntypes of nodes: decision nodes, which specify some test on a single attribute, and leaf no", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 60, "start": 40800, "end": 41600, "page": 19}
{"text": "tion. A decision tree has two\ntypes of nodes: decision nodes, which specify some test on a single attribute, and leaf nodes,\nwhich indicate the class. Since the CLTree algorithm is designed for numerical data, the\ntree algorithm performs a binary split, i.e., the current space is cut into two parts. For details\nabout the decision tree construction algorithm and its improvements, readers are referred\nto (Quinlan, 1993), (Shafer et al., 1996), (Gehrke et al., 1998), and (Gehrke et al., 1999).\nDuring the decision tree construction, a different number ofN points is added to each\nnode, but not physically. In order to separate cluster regions and noncluster regions, the\nnumber of N points is added according to some rules. That is, if the number of N points\ninherited from the parent node ofEis le", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 61, "start": 41480, "end": 42280, "page": 19}
{"text": "of N points is added according to some rules. That is, if the number of N points\ninherited from the parent node ofEis less than the number ofY points inE, then the number\nof N points for E is increased to the number of Y points in E; otherwise, the number of\ninherited N points is used for E.\nIn the CLTree algorithm, the decision tree algorithm is modified, since the N points\nare not physically added to the data. The first modification is to compute the number of N\npoints in each region based on the area of the region when a cut is made, since theN points\nare assumed to be distributed uniformly in the current space. For example, a node E has\n25 Y points and 25 N points and a cut P cuts E into two regions E\n1 and E2 in the ratio\n2:3, i.e., the area of E1 is 40% of the area of E and the area ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 62, "start": 42160, "end": 42960, "page": 19}
{"text": " a cut P cuts E into two regions E\n1 and E2 in the ratio\n2:3, i.e., the area of E1 is 40% of the area of E and the area of E2 is 60% of the area of E.\nThen the number of N points in E1 is 0.4 ×25 =10 and the number of N points in E2 is\n25 −10 =15. The second modification is to evaluate on both sides of data points in order\nto find the best cuts.\nAlso, a new criterion is used in order to find the best cuts, since the gain criterion has\nproblems in clustering, such as the cut with best information gain tends to cut into clusters.\nTo do this, the relative density of a region is defined. Let E be a region. Then the relative\ndensity of E is E.Y\nE.N , where E.Y is the number ofY points in E and E.N is defined similarly.\nThe\nbasic idea of the new criterion is described as follows. For each dimens", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 63, "start": 42840, "end": 43640, "page": 19}
{"text": "Y points in E and E.N is defined similarly.\nThe\nbasic idea of the new criterion is described as follows. For each dimensionj of a node\nE, let dj_cut1 be the first cut found by the gain criterion, and let the two regions separated\nby dj_cut1 along dimension j be denoted by Lj and Hj, where Lj has a lower relative\ndensity than Hj; then we have E =Lj ∪Hj. Let dj_cut2 be the cut of Lj found using the\ngain criterion. Suppose the region Lj is cut into L1\nj and H1\nj by dj_cut2, where L1\nj\nhas a\nlower relative density than H1\nj ; then we have Lj =L1\nj\n∪H1\nj .I f H1\nj is the region between\ndj_cut1 and dj_cut2, then we stop and choose dj_cut2 as a best cut for dimension j of\nE. If the region between dj_cut1 and dj_cut2i sL 1\nj\n, then we let dj_cut3 be the cut of L1\nj\nfound using the gain criterion a", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 64, "start": 43520, "end": 44320, "page": 19}
{"text": "the region between dj_cut1 and dj_cut2i sL 1\nj\n, then we let dj_cut3 be the cut of L1\nj\nfound using the gain criterion and choose dj_cut3 as the best cut for dimension j of E.\nAfter the decision tree has been constructed, it is pruned in order to produce meaningful\nclusters. CLTree provides two pruning methods: browsing and user-oriented pruning. In\nthe browsing method, a user interface is built in order to let the user simply explore the tree\nto find meaningful clusters. In the user-oriented pruning method, the decision tree is pruned\nusing two user-specified parameters min_y and min_rd: min_y is the percentage of the\ntotal number of data points in the data set that a region must contain, andmin_rd specifies\nwhether an N region (node) E should join an adjacent region F to form a bigger cl", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 65, "start": 44200, "end": 45000, "page": 20}
{"text": "gion must contain, andmin_rd specifies\nwhether an N region (node) E should join an adjacent region F to form a bigger cluster\nregion.\n15.9 PART\nPART (Projective Adaptive Resonance Theory) (Cao and Wu, 2002) is a new neural network\narchitecture that was proposed to find projected clusters for data sets in high-dimensional\nspaces. The neural network architecture in PART is based on the well known ART (Adaptive\nResonance Theory) developed by Carpenter and Grossberg (1987b,a, 1990). In PART, a\nso-called selective output signaling mechanism is provided in order to deal with the inherent\nsparsity in the full space of the high-dimensional data points. Under this selective output\nsignaling mechanism, signals generated in a neural node in the input layer can be transmitted\nto a neural node in the c", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 66, "start": 44880, "end": 45680, "page": 20}
{"text": "\nsignaling mechanism, signals generated in a neural node in the input layer can be transmitted\nto a neural node in the clustering layer only when the signal is similar to the top-down weight\nbetween the two neural nodes; hence, with this selective output signaling mechanism, PART\nis able to find dimensions where subspace clusters can be found.\nThe basic PART architecture consists of three components: the input layer (compar-\nison layer) F\n1, the clustering layer F2, and a reset mechanism (Cao and Wu, 2002). Let\nthe nodes in the F1 layer be denoted by vi,i = 1,2,...,m ; the nodes in the F2 layer be\ndenoted by vj,j = m +1,...,m +n; the activation of an F1 node vi be denoted by xi;\nand the activation of an F2 node vj be denoted by xj. Let the bottom-up weight from vi to\nvj be denoted by zij a", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 67, "start": 45560, "end": 46360, "page": 20}
{"text": " by xi;\nand the activation of an F2 node vj be denoted by xj. Let the bottom-up weight from vi to\nvj be denoted by zij and the top-down weight from vj to vi be denoted by zji.I n P A R T ,\nthe selective output signal of an F1 node vi to a committed F2 node vj is defined by\nhij =h(xi,zij,zji) =hσ(f1(xi),zji)l(zij), (15.13)\nwhere f1 is a signal function; hσ(·,·)is defined as\nhσ(a,b) =\n{ 1i f d(a,b) ≤σ,\n0i f d( a,b )>σ, (15.14)\nwith d(\na,b) being a quasi-distance function; and l(·)is defined as\nl(zij) =\n{ 1i f zij >θ ,\n0i f zij ≤θ, (15.15)\nwith θ being 0 or a small number to be specified as a threshold; σ is a distance parameter.\nAn F1 node vi is said to be active to vj if hij =1 and inactive to vj if hij =0.\nIn PART, anF2 node vj is said to be a winner either if poammaE̸=φ and Tj =max poamma", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 68, "start": 46240, "end": 47040, "page": 20}
{"text": "ij =1 and inactive to vj if hij =0.\nIn PART, anF2 node vj is said to be a winner either if poammaE̸=φ and Tj =max poammaEor if\npoammaE=φ and node vj is the next noncommitted node in the F2 layer, where poammaEis a set defined\nas poammaE={Tk :F2 node vk is committed and has not been reset on the current trial}, with Tk\ndefined as\nTk =\n∑\nvi∈F1\nzikhik =\n∑\nvi∈F1\nzikh(xi,zik,zki). (15.16)\nA winning F2 node will become active and all other F2 nodes will become inactive,\nsince the F2 layer makes a choice by a winner-take-all paradigm:\nf2(xj) =\n{ 1 if node vj is a winner,\n0 otherwise. (15.17)\nFor the vigilance and reset mechanism of PART, if a winning (active)F2 node vj does\nnot satisfy some vigilance conditions, it will be reset so that the node vj will always be\ninactive during the rest of the c", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 69, "start": 46920, "end": 47720, "page": 21}
{"text": "satisfy some vigilance conditions, it will be reset so that the node vj will always be\ninactive during the rest of the current trial. The vigilance conditions in PART also control\nthe degree of similarity of patterns grouped in the same cluster. A winning F2 node vj will\nbe reset if and only if\nrj <ρ, (15.18)\nwhere ρ ∈{1,2,...,m }is a vigilance parameter and rj is defined as\nrj =\n∑\ni\nhij. (15.19)\nTherefore, the vigilance parameter ρ controls the size of the subspace dimensions, and the\ndistance parameter σ controls the degree of similarity in the specific dimension involved.\nFor real-world data, it is a challenge to choose the distance parameter σ.\nIn PART, the learning is determined by the following formula. For a committed F2\nnode vj that has passed the vigilance test, the new bottom-up ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 70, "start": 47600, "end": 48400, "page": 21}
{"text": "s determined by the following formula. For a committed F2\nnode vj that has passed the vigilance test, the new bottom-up weight is defined as\nznew\nij =\n{ L\nL−1+|X| if F1 node vi is active to vj,\n0i f F1 node vi is inactive to vj, (15.20)\nwhere Lis a constant and |X|denotes the number of elements in the set X ={i :hij =1},\nand the new top-down weight is defined as\nznew\nji =(1 −α)zold\nji +αIi, (15.21)\nwhere 0 ≤α ≤1 is the learning rate.\nFor\na noncommitted winner vj, and for everyF1 node vi, the new weights are defined\nas\nznew\nij = L\nL−1 +m, (15.22)\nznew\nji =Ii. (15.23)\nIn PART, each committedF2 node vj represents a subspace cluster Cj. Let Dj be the\nset of subspace dimensions associated with Cj. Then i ∈Dj if and only if l(zij) =1, i.e.,\nthe set Dj is determined by l(zij).\nPART is very effect", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 71, "start": 48280, "end": 49080, "page": 21}
{"text": "s associated with Cj. Then i ∈Dj if and only if l(zij) =1, i.e.,\nthe set Dj is determined by l(zij).\nPART is very effective at finding the subspaces in which clusters are embedded, but\nthe difficulty of choosing some of its parameters restricts its application. For example, it\nis very difficult for users to choose an appropriate value for the distance parameter σ.I f\nwe choose a relatively small value the algorithm may not capture the similarities of two\nsimilar data points; however, if we choose a relatively large value, the algorithm may not\ndifferentiate two dissimilar data points.\n15.10 SUBCAD\nAll the subspace clustering algorithms mentioned before are designed for clustering numer-\nical data. SUBCAD (SUBspace clustering for high-dimensional CAtegorical Data) (Gan\nand Wu, 2004; Gan, 20", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 72, "start": 48960, "end": 49760, "page": 22}
{"text": "ustering numer-\nical data. SUBCAD (SUBspace clustering for high-dimensional CAtegorical Data) (Gan\nand Wu, 2004; Gan, 2003) is a subspace clustering algorithm designed for clustering high-\ndimensional categorical data. In the SUBCAD algorithm, the clustering process is treated\nas an optimization problem. An objective function is defined in SUBCAD to measure the\nquality of the clustering. The main goal of SUBCAD is to find an approximation of the\noptimal solution based on the objective function.\nGiven a data setD, let Qbe the set of dimensions ofD, i.e., Q ={1,2,...,d }, where\nd is the number of dimensions of D, and let Span(Q) denote the full space of the data set.\nThen by a subspace cluster we mean a clusterC associated with a set of dimensionsP such\nthat\n1. the data points in C are “simi", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 73, "start": 49640, "end": 50440, "page": 22}
{"text": "y a subspace cluster we mean a clusterC associated with a set of dimensionsP such\nthat\n1. the data points in C are “similar” to each other in the subspace Span(P)of Span(Q)\n(i.e., the data points in C are compact in this subspace);\n2. the data points in C are sparse in the subspace Span(R), where R =Q\\P (i.e., the\ndata points in C are spread in this subspace).\nIn SUBCAD, a subspace cluster is denoted by a pair(C,P) (P ̸=pνhiE), whereP is the\nnonempty set of dimensions associated with C. In particular, if P =Q, then this cluster is\nformed in the whole space of the data set. Therefore, if C is a cluster with the associated\nset of dimensions P, then C is also a cluster in every subspace of Span(P). Hence, a good\nsubspace clustering algorithm should be able to find clusters and the maximum ass", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 74, "start": 50320, "end": 51120, "page": 22}
{"text": "ery subspace of Span(P). Hence, a good\nsubspace clustering algorithm should be able to find clusters and the maximum associated\nset of dimensions. Consider, for example, the data set with five six-dimensional data points\ngiven in Table 2.1. In this data set, it is obvious that C ={ x\n1,x2,x3} is a cluster and\nthe maximum set of dimensions should be P ={ 1,2,3,4}. A good subspace clustering\nalgorithm should be able to find this cluster and the maximum set of associated dimensions\nP.\nThe\nobjective function is defined in terms of the compactness and separation of each\nsubspace cluster. Let (C,P) be a subspace cluster. Then the compactness Cp(C,P) and\nthe separation Sp(C,R) of cluster (C,P) are defined as\nCp(C,P) =\n2 ∑\nx,y∈C\n∥x−y∥2\nP\n|P||C|2 , (15.24)\nSp(C,P) =\n\n\n\n2 ∑\nx,y∈C\n∥x−y∥2\nR\n|R||C|2", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 75, "start": 51000, "end": 51800, "page": 22}
{"text": "f cluster (C,P) are defined as\nCp(C,P) =\n2 ∑\nx,y∈C\n∥x−y∥2\nP\n|P||C|2 , (15.24)\nSp(C,P) =\n\n\n\n2 ∑\nx,y∈C\n∥x−y∥2\nR\n|R||C|2 if R ̸=pνhiE,\n1i f R =pνhiE,\n(15.25)\nwhere |P|and |R|denote the number of elements in the setsP and R, respectively; ∥x−y∥P\ndenotes the distance between x and y in the subspace spanned by the dimensions inP; and\n∥x−y∥R denotes the distance in the subspace spanned by the dimensions in R.\nLet C1,C2,...,C k be a partition of the data set, where k is the number of clusters.\nLet Pj be the set of dimensions associated with clusterCj. The objective function is defined\nas\nFobj =\nk∑\nj=1\n(Cp(Cj,Pj)+1 −Sp(Cj,Rj)). (15.26)\nGiven the number of clusters k, the goal of SUBCAD is to partition the data set into\nk nonoverlapping groups such that the objective function Fobj defined in equa", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 76, "start": 51680, "end": 52480, "page": 23}
{"text": "f SUBCAD is to partition the data set into\nk nonoverlapping groups such that the objective function Fobj defined in equation (15.26)\nis minimized. The algorithm finds an approximation of the optimal partition.\nLet C be a cluster associated with the setP of dimensions andTf(C) be its frequency\ntable (cf. Section 2.1). Since the square of the simple matching distance is equal to itself,\nwe have\n∑\nx,y∈C\n∥x−y∥2\nP =\n∑\nx,y∈C\n∑\nj∈P\nδ(xj,yj)2\n=\n∑\nj∈P\n∑\nx,y∈C\nδ(xj,yj)\n=\n∑\nj∈P\n∑\n1≤r<s≤nj\nfjr (C) ·fjs(C)\n=\n∑\nj∈P\n1\n2\n\n\n( nj∑\nr=1\nfjr (C)\n)2\n−\nnj∑\nr=1\nf2\njr (C)\n\n\n= 1\n2|P|·| C|2 − 1\n2\n∑\nj∈P\nnj∑\nr=1\nf2\njr (C)\n= 1\n2|P|·| C|2 − 1\n2\n∑\nj∈P\n∥fj(C)∥2, (15.27)\nwhere δ(·,·)is the simple matching distance, and fj(C) is defined in equation (2.3).\nThus\nfrom equation (15.27), we obtain the following simplified fo", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 77, "start": 52360, "end": 53160, "page": 23}
{"text": "hing distance, and fj(C) is defined in equation (2.3).\nThus\nfrom equation (15.27), we obtain the following simplified formulas of compact-\nness and separation:\nCp(C,P) =1 −\n∑\nj∈P\n∥fj(C)∥2\n|P||C|2 , (15.28)\nSp(C,R) =\n\n\n\n1 −\n∑\nj∈R\n∥fj(C)∥2\n|R||C|2 if R ̸=pνhiE,\n1i f R =pνhiE.\n(15.29)\nThe SUBCAD algorithm consists of two phases: the initialization phase and the\noptimization phase. In the initialization phase, an initial partition is made. Good initial\npartition leads to fast convergence of the algorithm, while bad initial partition may make\nthe algorithm converge very slowly. Some initialization methods have been proposed in the\nliterature of clustering, such as the cluster-based method (Bradley and Fayyad, 1998) and\nthe kd-tree–based method (Pelleg and Moore, 1999). A sketch of the algori", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 78, "start": 53040, "end": 53840, "page": 23}
{"text": "er-based method (Bradley and Fayyad, 1998) and\nthe kd-tree–based method (Pelleg and Moore, 1999). A sketch of the algorithm is described\nin Algorithm 15.11.\nAlgorithm 15.11. The SUBCAD algorithm.\nRequire: D - Data Set, k - Number of Clusters;\nEnsure: 2 ≤k ≤|D|;\n1: if D is a large data set then\n2: Draw a sample from D;\n3: end if\n4: Compute the proximity matrix from the whole data set or the sample;\n5: Pick k most dissimilar data points as seeds;\n6: Assign the remaining data points to the nearest seed;\n7: repeat\n8: for i =1t o|D |do\n9: Let (Cl,Pl)be the subspace cluster that contains xi;\n10: for m =1,m ̸=l to k do\n11: if inequality (15.35) is true then\n12: Move x from Cl to Cm;\n13: Update subspaces Pl and Pm;\n14: end if\n15: end for\n16: end for\n17: until No further change in the cluster membe", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 79, "start": 53720, "end": 54520, "page": 24}
{"text": " Cm;\n13: Update subspaces Pl and Pm;\n14: end if\n15: end for\n16: end for\n17: until No further change in the cluster memberships;\n18: Output results.\nInitialization\nIn the initialization phase, the k most dissimilar data points are picked as seeds, where k\nis the number of clusters, and then the remaining data points are assigned to the nearest\nseed.\nLet D be a data set and k be a positive integer such that k ≤| D|. We say that\nx1,x2,...,x k ∈D are the k most dissimilar data points of the data set D if the following\ncondition is satisfied:\nmin\n1≤r<s≤k\n∥xr −xs∥=max\nE∈F\nmin\nx,y∈E\n∥x−y∥, (15.30)\nwhere F is the class that contains all subsets E of D such that |E|=k , i.e.,\nF ={E :E ⊆D,|E|=k },\nand ∥x−y∥is the distance between x and y.\nLet X(k,D) (k ≤| D|) denote the set of the k most dissimilar ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 80, "start": 54400, "end": 55200, "page": 24}
{"text": "E :E ⊆D,|E|=k },\nand ∥x−y∥is the distance between x and y.\nLet X(k,D) (k ≤| D|) denote the set of the k most dissimilar data points of the\ndata set D. Since there is a total of\n(\nn\nk\n)\nelements in F, when n is large, it is impractical\nto enumerate all sets in F to find the set of the k most dissimilar data points. Thus an\napproximation algorithm is employed to find a set of k data points that is near the set of\nthe k most dissimilar data points. The basic idea is to choose k initial data points and\nthen continuously replace the bad data points with good ones until no further changes are\nnecessary.\nMore specifically, let D ={x1,x2,...,x n}be a data set or a sample from a data set.\nFirst, let X(k,D) be {x1,x2,...,x k}, and let xr and xs (1 ≤r<s ≤k) be such that\n∥xr −xs∥= min\nx,y∈X(k,D)\n∥x−y∥", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 81, "start": 55080, "end": 55880, "page": 25}
{"text": "ata set.\nFirst, let X(k,D) be {x1,x2,...,x k}, and let xr and xs (1 ≤r<s ≤k) be such that\n∥xr −xs∥= min\nx,y∈X(k,D)\n∥x−y∥. (15.31)\nSecondly, for each of the data points x ∈D\\X(k,D) , let\nSr = min\ny∈(X(k,D)\\{x s})\n∥x−y∥, (15.32)\nSs = min\ny∈(X(k,D)\\{x r})\n∥x−y∥. (15.33)\nThen if Sr > ∥xr −xs∥, we letX(k,D) ∪{x}\\{xs}replace X(k,D) ;i f Ss > ∥xr −xs∥,w e\nlet X(k,D) ∪{x}\\{xr}replace X(k,D) .\nOptimization\nIn the optimization phase, the data points are reassigned in order to minimize the objective\nfunction (15.26) and update the subspaces associated with those clusters whose membership\nhas changed. Let (C\n1,P1),(C2,P2),...,(C k,Pk)be a partition of the data setD, and let x\nbe a data point in the subspace cluster(Cl,Pl). To achieve the membership changing rules,\nthe “exact assignment test” (Wishart,", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 82, "start": 55760, "end": 56560, "page": 25}
{"text": "ta point in the subspace cluster(Cl,Pl). To achieve the membership changing rules,\nthe “exact assignment test” (Wishart, 2002) technique is used in the optimization phase. x\nwill be moved from subspace cluster(Cl,Pl)to another subspace cluster(Cm,Pm)(m ̸=l)\nif the resulting cost function decreases, i.e., if the following inequality is true:\nk∑\ni=1\n(Cp(Ci)+1 −Sp(Ci)) >\nk∑\ni=1,i̸=l,m\nCp(Ci)\n+Cp(Cl −x)+1 −Sp(Cl −x)+Cp(Cm +x)\n+1 −Sp(Cm +x), (15.34)\nwhere Cl −x means Cl\\{x}Cm +x means Cm ∪{x}.\nThe inequality (15.34) is equivalent to\nCp(Cl)−Cp(Cl −x)−Sp(Cl)+Sp(Cl −x)\n> −Cp(Cm)+Cp(Cm +x)+Sp(Cm)−Sp(Cm +x). (15.35)\nDetermining Subspace Dimensions\nDuring the optimization phase of the algorithm, if a data point is moved from its current\ncluster to another one, then the subspace dimensions associated ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 83, "start": 56440, "end": 57240, "page": 25}
{"text": "he algorithm, if a data point is moved from its current\ncluster to another one, then the subspace dimensions associated with the two clusters should\nbe updated. In the SUBCAD algorithm, the set of subspace dimensions associated with\neach cluster is determined by an objective function.\nLet (C,E) be a subspace cluster of ad-dimensional data set D. In order to determine\nthe set P of subspace dimensions associated with C, an objective function whose domain\nis all the subsets of Q ={1,2,...,d }is defined as\nF(C,E) =Cp(C,E) +1 −Sp(C,Q\\E), pνhiE̸=E ⊆Q, (15.36)\nwhere Cp(C,E) and Sp(C,Q\\E) are the compactness and separation of cluster C under\nthe subspace dimensions set E.\nOur general idea to determine the set P associated with the cluster C is to find a\nP such that the objective function defined i", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 84, "start": 57120, "end": 57920, "page": 26}
{"text": "eral idea to determine the set P associated with the cluster C is to find a\nP such that the objective function defined in equation (15.36) is minimized. Also, from\nequation (15.27), if P ̸=pνhiEor P ̸=Q, the objective function in equation (15.36) can be\nwritten as\nF(C,E) =1 −\n∑\nj∈E\n∥fj(C)∥2\n|E||C|2 +\n∑\nj∈Q\\E\n∥fj(C)∥2\n|Q\\E||C|2 ,E ⊆Q, (15.37)\nwhere R =Q\\P.\nThe objective function defined in (15.37) has the following properties.\nTheorem 15.1 (Condition of constancy).The objective function defined in equation (15.36)\nis constant for any subset E of Q if and only if\n∥f1(C)∥=∥ f2(C)∥=···=∥ fd(C)∥. (15.38)\nIn addition, if the objective function is a constant, then it is equal to 1.\nFrom the definition of the objective functionF(C,E) , the proof of the above theorem\nis straightforward and is thus ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 85, "start": 57800, "end": 58600, "page": 26}
{"text": " 1.\nFrom the definition of the objective functionF(C,E) , the proof of the above theorem\nis straightforward and is thus omitted. Thus, from Theorem 15.1, if the objective function is\nconstant, then it is minimized at any subset ofQ. In this case, the set of subspace dimensions\nassociated with C is defined to be Q. If the objective function is not constant, then the set\nof subspace dimensions associated with C is defined to be the set P ⊆ Q that minimizes\nthe objective function. In fact, it can be shown that such a setP is unique if the objective is\nnot constant (Gan, 2003). Hence, the following definition of subspace associated with each\ncluster is well defined.\nDefinition\n15.2. Let C be a cluster. Then the setP of subspace dimensions associated with\nC is defined as follows:\n1. If the obje", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 86, "start": 58480, "end": 59280, "page": 26}
{"text": "n\n15.2. Let C be a cluster. Then the setP of subspace dimensions associated with\nC is defined as follows:\n1. If the objective function F(C,E) is constant for any E ⊆Q, then let P =Q.\n2. If the objective function F(C,E) is not constant, then P is defined as\nP =arg max\nE∈E\n|E|, (15.39)\nwhere E is defined as\nE ={O :F(C,O) =min\nE∈א\nF(C,E),O ∈א} (15.40)\nand is defined as\nא={ E :E ⊂Q,E ̸=pνhiE,E̸=Q}. (15.41)\nFrom Definition 15.2, the set P defined in equation (15.39) is nonempty. Moreover,\nif the objective function F(C,E) is not constant, then the set P is a true subset of Q, i.e.,\nP ⫋ Q. The objective function in (15.37) has many good properties, which will be given\nas theorems or corollaries as follows. Readers may refer to Gan (2003) for detailed proofs.\nTheorem 15.3. Let (C,P) be a subspace ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 87, "start": 59160, "end": 59960, "page": 27}
{"text": "s or corollaries as follows. Readers may refer to Gan (2003) for detailed proofs.\nTheorem 15.3. Let (C,P) be a subspace cluster of a d-dimensional data set D (d> 2),\nand let P be defined in equation (15.39). Then we have the following:\n1. Let r ∈P. If there exists an s( 1 ≤s ≤d) such that ∥fs(C)∥> ∥fr(C)∥, then s ∈P.\n2. Let r ∈R. If there exists an s( 1 ≤s ≤d) such that ∥fs(C)∥< ∥fr(C)∥, then s ∈R,\nwhere R =Q\\P.\nCorollary 15.4 (Monotonicity). Let (C,P) be a subspace cluster of D, where P is the set\nof subspace dimensions defined in equation (15.39) and letTf(C) be the frequency table of\nC. Then for any r ∈P and s ∈R(R =Q\\P), we have\n∥fr(C)∥≥∥ fs(C)∥. (15.42)\nTheorem 15.5. Let (C,P) be a subspace cluster of a d-dimensional data set D (d> 2),\nwhere P is defined in equation (15.39). Let Tf(C)", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 88, "start": 59840, "end": 60640, "page": 27}
{"text": " Let (C,P) be a subspace cluster of a d-dimensional data set D (d> 2),\nwhere P is defined in equation (15.39). Let Tf(C) be the frequency table and let r,s (1 ≤\nr,s ≤ d) be given so that ∥fs(C)∥=∥ fr(C)∥. Then either r,s ∈ P or r,s ∈ R, i.e., r,s\nmust be in the same set P or R, where R =Q\\P.\nCorollary 15.6. Let (C,P) be a subspace cluster of D, where P is the set of subspace\ndimensions defined in equation (15.39), and let Tf(C) be the frequency table of C. If the\nobjective function F(C,E) is not constant, then for any r ∈P and s ∈R( R= Q\\P),w e\nhave\n∥fr(C)∥> ∥fs(C)∥. (15.43)\nTheor\nem 15.7 (Uniqueness of subspace). Let F(C,E) be the objective function defined in\nequation (15.36) and let P be the set defined in equation (15.39). If the objective function\nF(C,E) is not constant, then the set ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 89, "start": 60520, "end": 61320, "page": 27}
{"text": "15.36) and let P be the set defined in equation (15.39). If the objective function\nF(C,E) is not constant, then the set P is unique.\nTheorem 15.8 (Contiguity). Let (C,P) be a subspace cluster of D, where P is the set of\nsubspace dimensions defined in equation (15.39). Let Tf(C) be the frequency table of C,\nand let i1,i2,...,i d be a combination of 1,2,...,d such that\n∥fi1 (C)∥≥∥ fi2 (C)∥≥···≥∥ fid (C)∥.\nFinally, let be the set of subscripts defined as\nGs =\n{\nt :∥fit (C)∦=∥fit+1 (C)∥,1 ≤t ≤d −1\n}\n. (15.44)\nIf the objective function defined in equation (15.36) is not constant, then the set of subspace\ndimensions P defined in equation (15.39) must be one of the Pk’s (k = 1,2,...,|G s|)\ndefined as\nPk ={it :t =1,2,...,g k},k =1,2,...,|G s|, (15.45)\nwhere g1 <g 2 < ··· <g |Gs| are elements of Gs", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 90, "start": 61200, "end": 62000, "page": 28}
{"text": ",...,|G s|)\ndefined as\nPk ={it :t =1,2,...,g k},k =1,2,...,|G s|, (15.45)\nwhere g1 <g 2 < ··· <g |Gs| are elements of Gs.\nBased on Theorem 15.8, the set of subspace dimensions P for a cluster C can be\nfound very quickly. There are 2d −1 nonempty subsets of Q, so it is impractical to find an\noptimal P by enumerating these 2d −1 subsets. Based on Theorem 15.8, a fast algorithm\nis possible for determining the set of subspace dimensions.\n15.11 Fuzzy Subspace Clustering\nIn fuzzy clustering algorithms, each object has a fuzzy membership associated with each\ncluster indicating the degree of association of the object to the cluster. In this section\nwe present a fuzzy subspace clustering (FSC) algorithm (Gan et al., 2006; Gan, 2006) for\nclustering high-dimensional data sets. In FSC each dimension h", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 91, "start": 61880, "end": 62680, "page": 28}
{"text": "stering (FSC) algorithm (Gan et al., 2006; Gan, 2006) for\nclustering high-dimensional data sets. In FSC each dimension has a fuzzy membership\nassociated with each cluster indicating the degree of importance of the dimension to the\ncluster (see Definition 15.9). Using fuzzy techniques for subspace clustering, FSC avoids the\ndifficulty of choosing appropriate subspace dimensions for each cluster during the iterations.\nDefinition 15.9 (Fuzzy dimension weight matrix). A k×d matrix W =(w\njh)is said to\nbe a fuzzy dimension weight matrix if W satisfies the following conditions:\nwjh ∈[0,1], 1 ≤j ≤k, 1 ≤h ≤d, (15.46a)\nd∑\nh=1\nwjh =1, 1 ≤j ≤k. (15.46b)\nThe set of all such fuzzy dimension weight matrices is denoted by Mfk , i.e.,\nMfk ={W ∈Rkd|W satisfies equation (15.46)}. (15.47)\nThe\nmain idea behind", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 92, "start": 62560, "end": 63360, "page": 28}
{"text": "nsion weight matrices is denoted by Mfk , i.e.,\nMfk ={W ∈Rkd|W satisfies equation (15.46)}. (15.47)\nThe\nmain idea behind the FSC algorithm is to impose weights on the distance measure\nof the k-means algorithm (Hartigan, 1975; Hartigan and Wong, 1979) in order to capture\nappropriate subspace information. To describe FSC, we first briefly describe the k-means\nalgorithm. Let the set of k centers be denoted by Z ={z1,z2,...,z k}. Then the objective\nfunction of the k-means algorithm is\nE =\nk∑\nj=1\n∑\nx∈Cj\n∥x−zj∥2, (15.48)\nwhere ∥·∥ is the Euclidean norm and Cj is the jth cluster. Alternatively, the objective\nfunction of k-means in equation (15.48) can be formulated as\nE =\nk∑\nj=1\nn∑\ni=1\nuji∥x−zj∥2,\nwhere U =(uji)is a hard k-partition (see Subsection 1.2.4) of D.\nLike locally adapative clustering (", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 93, "start": 63240, "end": 64040, "page": 28}
{"text": "\nn∑\ni=1\nuji∥x−zj∥2,\nwhere U =(uji)is a hard k-partition (see Subsection 1.2.4) of D.\nLike locally adapative clustering (LAC) (Domeniconi et al., 2004), FSC associates\nwith each cluster a weight vector in order to capture the subspace information of that cluster.\nFor example, the hth dimension is associated with the jth cluster to a degree of wjh or the\njth cluster has fuzzy dimension weights specified by wj = (wj1,wj2,...,w jd )T . The\nfuzzy dimension weights of the k clusters are represented by a fuzzy dimension weight\nmatrix W =(w1,w2,...,w k)T .\nMathematically, the objective function of the algorithm is formated by imposing\nweights on the distance measure in equation (15.48) as\nEα,ε(W,Z,U) =\nk∑\nj=1\nn∑\ni=1\nuji\nd∑\nh=1\nwα\njh(xih −zjh)2 +ε\nk∑\nj=1\nd∑\nh=1\nwα\njh, (15.49)\nwhere W, Z, and U are ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 94, "start": 63920, "end": 64720, "page": 29}
{"text": "15.48) as\nEα,ε(W,Z,U) =\nk∑\nj=1\nn∑\ni=1\nuji\nd∑\nh=1\nwα\njh(xih −zjh)2 +ε\nk∑\nj=1\nd∑\nh=1\nwα\njh, (15.49)\nwhere W, Z, and U are the fuzzy dimension weight matrix, the centers, and the hard k-\npartition of D, respectively; α ∈(1,∞)is a weight component or fuzzifier; and εis a very\nsmall positive real number. Note that any one of W, Z, and U can be determined from the\nother two. The following three theorems (Theorems 15.10 –15.12) show how to estimate\none of W, Z, and U from the other two such that the objective function E\nα,ε(W,Z,U) is\nminimized.\nFrom the definition of the objective function in equation (15.49), we see that there\nis a term ε∑k\nj=1\n∑d\nh=1\nwα\njh. As you will see while analyzing the FSC algorithm, this term\nis introduced in the objective function in order to avoid the divide-by-zero e", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 95, "start": 64600, "end": 65400, "page": 29}
{"text": "le analyzing the FSC algorithm, this term\nis introduced in the objective function in order to avoid the divide-by-zero error when\ncalculating W given the estimates of Z and U. The parameter εis specified to be a very\nsmall positive real number so that the term has very little impact on the objective function.\nThe following theorem is used to find a hard k-partition U given the estimates of\nW and Z such that the objective function Eα,ε(W,Z,U) defined in equation (15.49) is\nminimized.\nTheorem 15.10. Given an estimate W∗ of W and an estimate Z∗ of Z, then a hard k-\npartition U = (uji) minimizes the objective function Eα,ε(W∗,Z∗,U) if it satisfies the\ncondition\nuji =1 implies j ∈\n{\nr ∈{1,2,...,k }|r =arg min\n1≤l≤k\ndli\n}\n, (15.50)\nwhere dli =∑d\nh=1(w∗\nlh)α(xih −z∗\nlh)2. The corresponding cluste", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 96, "start": 65280, "end": 66080, "page": 29}
{"text": " j ∈\n{\nr ∈{1,2,...,k }|r =arg min\n1≤l≤k\ndli\n}\n, (15.50)\nwhere dli =∑d\nh=1(w∗\nlh)α(xih −z∗\nlh)2. The corresponding clusters Cj (j =1,2,...,k) are\nformulated as\nCj =\n{\nxi ∈D|uij =1, 1 ≤i ≤n\n}\n. (15.51)\nProof. We rearrange the objective function Eα,ε(W∗,Z∗,U) as\nEα,ε(W∗,Z∗,U) =\nn∑\ni=1\nk∑\nj=1\nuji\nd∑\nh=1\n(w∗\njh)α(xih −z∗\njh)2 +ε\nk∑\nj=1\nd∑\nh=1\n(w∗\njh)α\n=\nn∑\ni=1\nk∑\nj=1\nujidji +ε\nk∑\nj=1\nd∑\nh=1\n(w∗\njh)α,\nwhere dji =∑d\nh=1\n(w∗\njh)α(xih −z∗\njh)2. Since ∑k\nj=1\nujidji (i =1,2,...,n) are mutually\nuncorrelated and ε∑k\nj=1\n∑d\nh=1\n(w∗\njh)α is fixed, Eα,ε(W∗,Z∗,U) is minimized if each term∑k\nj=1\nujidji is minimized for i =1,2,...,n . Note that only one of u1i,u2i,...,u ki is 1\nand all others are zero. From this it follows immediately that ∑k\nj=1\nujidji is minimized if\nuji satisfies equation (15.50).\nEquatio", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 97, "start": 65960, "end": 66760, "page": 30}
{"text": "rs are zero. From this it follows immediately that ∑k\nj=1\nujidji is minimized if\nuji satisfies equation (15.50).\nEquation (15.51) follows from the fact that every point is assigned to its nearest center\nin terms of the weighted distance. This proves the theorem.\nFrom Theorem 15.10, we see that the objective function Eα,ε(W,Z,U) of the FSC\nalgorithm can be formulated as\nEα,ε(W,Z,U)\n=\nn∑\ni=1\nmin\n{ d∑\nh=1\nwα\njh(xih −zjh)2|j =1,2,...,k\n}\n+ε\nk∑\nj=1\nd∑\nh=1\nwα\njh, (15.52)\nsince only one of u1i,u2i,...,u ki is equal to 1 and the others are equal to zero.\nNote that the hard k-partition U calculated from equation (15.50) is not unique, since\na data point can be assigned to any center to which it has the nearest distance. A particular\nchoice of U must be made when implementing the FSC algorithm.\nThe ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 98, "start": 66640, "end": 67440, "page": 30}
{"text": "er to which it has the nearest distance. A particular\nchoice of U must be made when implementing the FSC algorithm.\nThe following theorem is used to find the cluster centers Z given the estimates of\nW and U such that the objective function Eα,ε(W,Z,U) defined in equation (15.49) is\nminimized.\nTheorem 15.11. Given an estimate W∗ of W and an estimate U∗ of U, then the objective\nfunction Eα,ε(W∗,Z,U ∗)is minimized if Z =(zjh) is calculated as\nzjh =\nn∑\ni=1\nu∗\njixih\nn∑\ni=1\nu∗\nji\n, 1 ≤j ≤k, 1 ≤h ≤d. (15.53)\nProof. T\no prove this theorem, we take partial derivatives of Eα,ε(W∗,Z,U ∗) with respect\nto zjhs, set them to zero, and solve the resulting system. That is,\n∂Eα,ε(W∗,Z,U ∗)\n∂zjh\n=\nn∑\ni=1\n−2u∗\nji(w∗\njh)α(xih −zjh) =0, 1 ≤j ≤k, 1 ≤h ≤d.\nThis gives\nzjh =\nn∑\ni=1\nu∗\nji(w∗\njh)αxih\nn∑\ni=1\nuji(w∗\njh", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 99, "start": 67320, "end": 68120, "page": 30}
{"text": "\n∂zjh\n=\nn∑\ni=1\n−2u∗\nji(w∗\njh)α(xih −zjh) =0, 1 ≤j ≤k, 1 ≤h ≤d.\nThis gives\nzjh =\nn∑\ni=1\nu∗\nji(w∗\njh)αxih\nn∑\ni=1\nuji(w∗\njh)α\n=\nn∑\ni=1\nu∗\nji\nxih\nn∑\ni=1\nu∗\nji\n, 1 ≤j ≤k, 1 ≤h ≤d. (15.54)\nThis proves the theorem.\nThe following theorem is used to find the fuzzy dimension weight matrix W given\nthe estimates of Z and U such that the objective function Eα,ε(W,Z,U) is minimized.\nTheorem 15.12. Given an estimate Z∗ of Z and an estimate U∗ of U, then the objective\nfunction Eα,ε(W,Z ∗,U ∗)is minimized if W =(wjh) is calculated as\nwjh = 1\nd∑\nl=1\n\n\nn∑\ni=1\nu∗\nji (xih−z∗\njh)2+ε\nn∑\ni=1\nu∗\nji\n(xil−z∗\njl )2+ε\n\n\n1\nα−1\n, 1 ≤j ≤k, 1 ≤h ≤d. (15.55)\nProof. To prove this theorem, we use the method of Lagrange multipliers. To do this, let\nus first write the Lagrangian function as\nF(W,Z ∗,U ∗,pθambdaE)=Eα,ε(W,Z ∗", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 100, "start": 68000, "end": 68800, "page": 31}
{"text": "thod of Lagrange multipliers. To do this, let\nus first write the Lagrangian function as\nF(W,Z ∗,U ∗,pθambdaE)=Eα,ε(W,Z ∗,U ∗)−\nk∑\nj=1\nλj\n( d∑\nh=1\nwjh −1\n)\n.\nBy taking partial derivatives with respect to Wjh, we have\n∂F(W,Z ∗,U ∗,pθambdaE)\n∂wjh\n=\nn∑\ni=1\nαu∗\njiwα−1\njh (xih −z∗\njh)2 +εαwα−1\njh −λj\n=αwα−1\njh\n( n∑\ni=1\nu∗\nji\n(xih −z∗\njh)2 +ε\n)\n−λj\n=0 (15.56)\nfor 1 ≤j ≤k, 1 ≤h ≤d and\n∂F\n(W,Z ∗,U ∗,pθambdaE)\n∂λj\n=\nd∑\nh=1\nwjh −1 =0 (15.57)\nfor 1 ≤j ≤k.\nFrom equation (15.56), we have\nwjh =\n\n\nλj\nα\n( n∑\ni=1\nu∗\nji(xih −z∗\njh)2 +ε\n)\n\n\n1\nα−1\n, 1 ≤j ≤k, 1 ≤h ≤d. (15.58)\nCombining equation (15.57) and equation (15.58) gives\nd∑\nh=1\n\n\nλj\nα\n( n∑\ni=1\nu∗\nji(xih −z∗\njh)2 +ε\n)\n\n\n1\nα−1\n=1, 1 ≤j ≤k,\nor\nλ\n1\nα−1\nj = 1\nd∑\nh=1\n\n 1\nα\n( n∑\ni=1\nu∗\nji (xih−z∗\njh)2+ε\n)\n\n\n1\nα−1\n, 1 ≤j ≤k. (15.59)\nPlugging", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 101, "start": 68680, "end": 69480, "page": 31}
{"text": "1\nα−1\n=1, 1 ≤j ≤k,\nor\nλ\n1\nα−1\nj = 1\nd∑\nh=1\n\n 1\nα\n( n∑\ni=1\nu∗\nji (xih−z∗\njh)2+ε\n)\n\n\n1\nα−1\n, 1 ≤j ≤k. (15.59)\nPlugging λj from equation (15.59) into equation (15.58), we have\nwjh = 1\nd∑\nl=1\n\n\nn∑\ni=1\nu∗\nji (xih−z∗\njh)2+ε\nn∑\ni=1\nu∗\nji\n(xil−z∗\njl )2+ε\n\n\n1\nα−1\n, 1 ≤j ≤k, 1 ≤h ≤d.\nThis proves the theorem.\nAlgorithm 15.12. The pseudocode of the FSC algorithm.\nRequire: D - the data set, k - the number of clusters, and α - the fuzzifier;\n1: Initialize Z by choosing k points from D randomly;\n2: Initialize W with wjh = 1\nd (1 ≤j ≤k, 1 ≤h ≤d);\n3: Estimate U from initial values of W and Z according to equation (15.50);\n4: Let error =1 and Obj =Eα,ε(W,Z);\n5: while error > 0 do\n6: Update Z according to Theorem 15.11;\n7: Update W according to Theorem 15.12;\n8: Update U according to Theorem 15.10;\n9", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 102, "start": 69360, "end": 70160, "page": 32}
{"text": ": Update Z according to Theorem 15.11;\n7: Update W according to Theorem 15.12;\n8: Update U according to Theorem 15.10;\n9: Calculate NewObj =Eα,ε(W,Z);\n10: Let error =|NewObj −Obj|, and then Obj ⇐NewObj ;\n11: end while\n12: Output W, Z, and U.\nWe see from equation (15.53) and equation (15.55) that FSC is very similar to the\nfuzzy k-means algorithm (Huang and Ng, 1999) in terms of the way they update centers and\nfuzzy memberships. Like the fuzzy k-means algorithm, FSC is implemented recursively\n(see Algorithm 15.12). FSC starts with initial estimates of Z, W, and the partition U\ncalculated from Z and W, and then repeats estimating the centers Z given the estimates of\nW and U, estimating the fuzzy dimension weight matrix W given the estimates of Z and\nU, and estimating the partition U given th", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 103, "start": 70040, "end": 70840, "page": 32}
{"text": ", estimating the fuzzy dimension weight matrix W given the estimates of Z and\nU, and estimating the partition U given the estimates of W and Z, until it converges.\n15.12 Mean Shift for Subspace Clustering\nThis section presents a novel approach, the mean shift for subspace clustering (MSSC)\nalgorithm (Gan, 2006), to identifying subspace clusters. First, we introduce the relation-\nship among several clustering algorithms: the mean shift algorithm, the maximum-entropy\nclustering (MEC) algorithm, the FSC algorithm, the k-means algorithm, and the MSSC al-\ngorithm. Then we briefly introduce the MSSC algorithm. Finally, we examine some special\ncases of the MSSC algorithm and compute the critical value forβ, a key parameter required\nby the algorithm, when the first phase transition occurs.\nFigure ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 104, "start": 70720, "end": 71520, "page": 33}
{"text": "pute the critical value forβ, a key parameter required\nby the algorithm, when the first phase transition occurs.\nFigure 15.1 shows the relationship between the mean shift algorithm and its deriva-\ntives. The k-means algorithm is a limit case of the MEC algorithm (Rose et al., 1990) which,\nin turn, is a special case of the mean shift algorithm (Cheng, 1995). The MSSC algorithm\nis an extension of the MEC algorithm for subspace clustering.\nThe main idea behind the MSSC algorithm is to impose weights on the distance\nmeasure of the MEC algorithm (Rose et al., 1990) in order to capture appropriate subspace\ninformation. Readers are referred to Section 9.7 for a brief review of the MEC algorithm.\nGiven a data set D ={x\n1,x2,...,x n}and a set of centers Z ={z1,z2,...,z k}, recall that\nthe free ener", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 105, "start": 71400, "end": 72200, "page": 33}
{"text": "he MEC algorithm.\nGiven a data set D ={x\n1,x2,...,x n}and a set of centers Z ={z1,z2,...,z k}, recall that\nthe free energy defined in the MEC algorithm is defined as\nF =− 1\nβ\n∑\nx∈D\nln\n\n\nk∑\nj=1\ne−β∥x−zj∥2\n\n,\nwhere β is a parameter called the Lagrange multiplier.\nLike the FSC algorithm (see Section 15.11), the MSSC algorithm associates with\neach cluster a weight vector in order to capture the subspace information of that clus-\nter.In particular, the hth dimension is associated with the jth cluster C\nj to a degree of wjh\nThe k-means algorithm\nThe MEC algorithm\nThe FSC algorithm\nThe MSSC algorithm\nThe mean shift algorithm\nW\nW\nβ →∞ β →∞\nFigure 15.1. The relationship between the mean shift algorithm and its derivatives.\nor the jth cluster has fuzzy dimension weights specified by wj = (wj1,wj", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 106, "start": 72080, "end": 72880, "page": 33}
{"text": "n the mean shift algorithm and its derivatives.\nor the jth cluster has fuzzy dimension weights specified by wj = (wj1,wj2,...,w jd )T .\nMathematically, the objective function of the MSSC algorithm is formulated by imposing\nweights on the distance measure in the free energy as\nFα,β,ε(W,Z)\n=− 1\nβ\nn∑\ni=1\nln\n\n\nk∑\nj=1\nexp\n(\n−β\nd∑\nh=1\nwα\njh(xih −zjh)2\n)\n+ε\nk∑\nj=1\nd∑\nh=1\nwα\njh\n=− 1\nβ\nn∑\ni=1\nln\n\n\nk∑\nj=1\ne−βdji (W,Z)\n\n+ε\nk∑\nj=1\nd∑\nh=1\nwα\njh, (15.60)\nwhere α ∈(1,∞)is the fuzzifier controlling the fuzzy dimension weights;β is a parameter\nranging from zero to infinity, i.e., β ∈ (0,∞); εis a very small positive number; W =\n(w1,w2,...,w k)T is the fuzzy dimension weight matrix; Z ={z1,z2,...,z k}is the set of\ncenters; and dji(W,Z) is defined as\ndji(W,Z) =\nd∑\nh=1\nwα\njh(xih −zjh)2, 1 ≤i ≤n, 1 ≤j ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 107, "start": 72760, "end": 73560, "page": 34}
{"text": "; Z ={z1,z2,...,z k}is the set of\ncenters; and dji(W,Z) is defined as\ndji(W,Z) =\nd∑\nh=1\nwα\njh(xih −zjh)2, 1 ≤i ≤n, 1 ≤j ≤k. (15.61)\nThe following theorem is used to find the set of centers Z given the estimate of W\nsuch that the objective function Fα,β,ε(W,Z) is minimized.\nTheor\nem 15.13. Let γ :Rkd →R be defined as\nγ(Z) =Fα,β,ε(W,Z), (15.62)\nwhere W ∈Mfk is fixed. Assume wjh > 0 for all 1 ≤j ≤k and 1 ≤h ≤d. Then the set\nof centers Z∗ optimizes the function γ if Z∗ satisfies the implicit equation\nz∗\njh =\nn∑\ni=1\nxihP(xi ∈Cj)\nn∑\ni=1\nP(xi ∈Cj)\n, 1 ≤j ≤k, 1 ≤h ≤d, (15.63a)\nor\nz∗\nj =\nn∑\ni=1\nxiP(xi ∈Cj)\nn∑\ni=1\nP(xi ∈Cj)\n=\nn∑\ni=1\nxiuji\nn∑\ni=1\nuji\n, 1 ≤j ≤k, (15.63b)\nwhere P(xi ∈ Cj) = uji is the fuzzy membership of xi associated with cluster Cj and is\ndefined as\nP(xi ∈Cj) =uji = e−βdji (W,Z∗)\nk∑\n", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 108, "start": 73440, "end": 74240, "page": 34}
{"text": " Cj) = uji is the fuzzy membership of xi associated with cluster Cj and is\ndefined as\nP(xi ∈Cj) =uji = e−βdji (W,Z∗)\nk∑\nl=1\ne−βdli(W,Z∗)\n, 1 ≤i ≤n, 1 ≤j ≤k, (15.64)\nwith dji(·,·) defined as in equation (15.61). In addition, for β = 0, the set of centers Z∗\nsatisfying equation (15.63) is the global minimum for Fα,0,ε.\nProof. Since minimization of γ over Rkd is an unconstrained problem, the necessity of\nequation (15.63) follows by requiring ∇Zγ to vanish. Equivalently, the directional deriva-\ntive ∂γ(Z ∗+tZ)\n∂Z vanishes at Z∗ in arbitrary directions Z ∈ Rkd,Z ̸=0. Let t ∈ R and\ndefine\nh(t) =γ(Z∗ +tZ) =− 1\nβ\nn∑\ni=1\nln\n\n\nk∑\nj=1\ne−βdji (W,Z∗+tZ)\n\n+ε\nk∑\nj=1\nd∑\nh=1\nwα\njh,\nwhere dji(·,·) is defined in equation (15.61). Rearranging the terms in dji in the above\nequation leads to\nh(t) =− 1\nβ\nn∑\n", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 109, "start": 74120, "end": 74920, "page": 35}
{"text": "ere dji(·,·) is defined in equation (15.61). Rearranging the terms in dji in the above\nequation leads to\nh(t) =− 1\nβ\nn∑\ni=1\nln\n\n\nk∑\nj=1\ne−β(aji t2+bji t+cji )\n\n+ε\nk∑\nj=1\nd∑\nh=1\nwα\njh,\nwhere aji,bji, and cji are defined as\naji =\nd∑\nh=1\nwα\njhz2\njh,\nbji =−2\nd∑\nh=1\nwα\njh(xih −z∗\njh)zjh,\ncji =\nd∑\nh=1\nwα\njh(xih −z∗\njh)2.\nTaking the derivative of h(t), we have\nh′(t) = dh(t)\ndt =\nn∑\ni=1\nk∑\nj=1\n(2ajit +bji)e−β(aji t2+bji t+cji )\nk∑\nj=1\ne−β(aji t2+bji t+cji )\n, (15.65)\nand\nh′(0) =\nn∑\ni=1\nk∑\nj=1\nbjie−βcji\nk∑\nj=1\ne−βcji\n=−2\nn∑\ni=1\nk∑\nj=1\nd∑\nh=1\nwα\njh(xih −z∗\njh)zjhe−βcji\nk∑\nj=1\ne−βcji\n=−2\nk∑\nj=1\nd∑\nh=1\nwα\njhzjh\nn∑\ni=1\n(xih −z∗\njh)e−βcji\nk∑\nl=1\ne−βcli\n=0.\nSince h′(0) =0 holds for arbitrary directions Z =(zjh), noting that wjh > 0 for all j,h,i t\nis necessary for every j,h that\nn∑\ni=1\n(xih −z∗\njh)e−", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 110, "start": 74800, "end": 75600, "page": 35}
{"text": "arbitrary directions Z =(zjh), noting that wjh > 0 for all j,h,i t\nis necessary for every j,h that\nn∑\ni=1\n(xih −z∗\njh)e−βcji\nk∑\nl=1\ne−βcli\n=0,\nfrom which equation (15.63) follows and the necessity is established.\nT\no prove that Z∗ is the global minimum of Fα,β,ε for β =0, we examine the kd ×kd\nHessian matrix Hγ(Z∗)of γ evaluated at Z∗. In fact, from equation (15.65), we have\nh′′(t) =\nn∑\ni=1\nk∑\nj=1\n[\n2aji −β(2ajit +bji)2]\ne−β(aji t2+bji t+cji )\nk∑\nj=1\ne−β(aji t2+bji t+cji )\n(\nk∑\nj=1\ne−β(aji t2+bji t+cji )\n)2\n−\nn∑\ni=1\n−β\n(\nk∑\nj=1\n(2ajit +bji)e−β(aji t2+bji t+cji )\n)2\n(\nk∑\nj=1\ne−β(aji t2+bji t+cji )\n)2 .\nThus, at t =0, noting that β =0 we have\nh′′(0)\n=(z11,z12,...,z kd)Hγ(Z∗)(z11,z12,...,z kd)T\n=\nn∑\ni=1\nk∑\nj=1\n[\n2aji −βb2\nji\n]\ne−βcji\nk∑\nj=1\ne−βcji +β\n(\nk∑\nj=1\nbjie−βcji\n)2\n( k∑\nl=1\ne−βcli\n)2\n=", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 111, "start": 75480, "end": 76280, "page": 36}
{"text": ")(z11,z12,...,z kd)T\n=\nn∑\ni=1\nk∑\nj=1\n[\n2aji −βb2\nji\n]\ne−βcji\nk∑\nj=1\ne−βcji +β\n(\nk∑\nj=1\nbjie−βcji\n)2\n( k∑\nl=1\ne−βcli\n)2\n=\nn∑\ni=1\nk∑\nj=1\n2ajie−βcji\nk∑\nj=1\ne−βcji +β\n\n\n(\nk∑\nj=1\nbjie−βcji\n)2\n−\nk∑\nj=1\nb2\njie−βcji\nk∑\nj=1\ne−βcji\n\n\n( k∑\nl=1\ne−βcli\n)2\n= 2\nk\nn∑\ni=1\nk∑\nj=1\naji\n> 0.\nHence, the Hessian matrix is positive definite, so Z∗ is the global minimum of Fα,0,ε. This\nproves the theorem.\nFrom Theorem 15.13 we see that atβ =0 there is only one cluster, sinceuji = 1\nk for\nall j,i and zj,j =1,2,...,k , are identical to the center of the data set. At higher β, the\nobjective function Fα,β,ε may have many local minima and the cluster will split into smaller\nclusters. Once we obtain a new set of centers, we need to update the fuzzy dimension weight\nfor the clusters. The following theorem tells how t", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 112, "start": 76160, "end": 76960, "page": 37}
{"text": "n a new set of centers, we need to update the fuzzy dimension weight\nfor the clusters. The following theorem tells how to do this.\nTheorem 15.14. Let η :Mfk →R be defined as\nη(W) =Fα,β,ε(W,Z), (15.66)\nwher\ne Z ∈Rkd is fixed. Then the fuzzy dimension weight W∗ optimizes the function ηif W∗\nsatisfies the implicit function\nw∗\njh = 1\nd∑\nl=1\n\n\nn∑\ni=1\nuji (xih−zjh)2+ε\nn∑\ni=1\nuji (xil−zjl )2+ε\n\n\n1\nα−1\n, 1 ≤j ≤k, 1 ≤h ≤d, (15.67)\nwhere the fuzzy membership uji is defined as\nuji =\nexp\n(\n−β\nd∑\nh=1\n(w∗\njh)α(xih −zjh)2\n)\nk∑\nl=1\nexp\n(\n−β\nd∑\nh=1\n(w∗\nlh)α(xih −zlh)2\n), 1 ≤j ≤k, 1 ≤i ≤n. (15.68)\nProof. Minimization of η over Mfk is a Kuhn-Tucker problem carrying the kd inequality\nconstraints (15.46a) and k equality constraints (15.46b) (Bezdek, 1980). This can be done\nvia the method of Lagrange multip", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 113, "start": 76840, "end": 77640, "page": 37}
{"text": "straints (15.46a) and k equality constraints (15.46b) (Bezdek, 1980). This can be done\nvia the method of Lagrange multipliers. Let pθambdaE= (λ1,λ2,...,λ k) be the multipliers and\npoammaE(W,pθambdaE)be the Lagrangian\npoammaE(W,pθambdaE)=− 1\nβ\nn∑\ni=1\nln\n\n\nk∑\nj=1\ne−βdji (W,Z)\n\n+ε\nk∑\nj=1\nd∑\nh=1\nwα\njh −\nk∑\nj=1\nλj\n( d∑\nh=1\nwjh −1\n)\n.\nIf (W∗,pθambdaE∗)is to minimize poammaE, its gradient in both sets of variables must vanish, i.e.,\n∂poammaE(W∗,pθambdaE∗)\n∂wjh\n=\nn∑\ni=1\nα(w∗\njh)α−1(xih −zjh)2e−βdji (W∗,Z∗)\nk∑\nj=1\ne−βdji (W∗,Z∗)\n+εα(w∗\njh)α−1 −λj\n=α(w∗\njh)α−1\n( n∑\ni=1\nuji(xih −zjh)2 +ε\n)\n−λj\n=0, 1 ≤j ≤k, 1 ≤h ≤d, (15.69a)\nwhere uji is defined in Equation (15.68), and\n∂poammaE(W∗,pθambdaE∗)\n∂λj\n=\nd∑\nh=1\nw∗\njh −1 =0, 1 ≤j ≤k. (15.69b)\nEquation (15.67) following immediately by solving the k(d +1)e", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 114, "start": 77520, "end": 78320, "page": 38}
{"text": "W∗,pθambdaE∗)\n∂λj\n=\nd∑\nh=1\nw∗\njh −1 =0, 1 ≤j ≤k. (15.69b)\nEquation (15.67) following immediately by solving the k(d +1)equations in (15.69a) and\n(15.69b). This proves the theorem.\nFrom equation (15.68), we see that uji > 0 for all 1 ≤ j ≤ k and 1 ≤ i ≤ n.\nIf the data set D is such that for each dimension h there exist two distinct values, then∑n\ni=1 uli(xih −zlh)2 > 0 for all 1≤l ≤kand 1 ≤h ≤d. In this case, we set the parameter\nε = 0,\nsince the purpose of ε is to avoid divide-by-zero errors when implementing the\nMSSC algorithm. In all analysis below, we assume ε=0.\nThe FSC algorithm presented in Section 15.11 is a limit case of the MSSC algorithm\npresented above when β →∞ (see Figure 15.1). In fact, letting β approach infinity, we\nhave\nlim\nβ→∞\nFα,β,ε(W,Z)\n= lim\nβ→∞\n1\n−1\nn∑\ni=1\nk∑\nj=1\n−dji", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 115, "start": 78200, "end": 79000, "page": 38}
{"text": "∞ (see Figure 15.1). In fact, letting β approach infinity, we\nhave\nlim\nβ→∞\nFα,β,ε(W,Z)\n= lim\nβ→∞\n1\n−1\nn∑\ni=1\nk∑\nj=1\n−dji(W,Z)e −βdji (W,Z)\nk∑\nj=1\ne−βdji (W,Z)\n+ε\nk∑\nj=1\nd∑\nh=1\nwα\njh\n=\nn∑\ni=1\nmin\n{\ndji(W,Z)|j =1,2,...,k\n}\n+ε\nk∑\nj=1\nd∑\nh=1\nwα\njh\n=\nn∑\ni=1\nmin\n{ d∑\nh=1\nwα\njh(xih −zjh)2|j =1,2,...,k\n}\n+ε\nk∑\nj=1\nd∑\nh=1\nwα\njh,\nwhich is exactly the objective function of the FSC algorithm defined in equation (15.52).\nClearly equation (15.63b) is an implicit equation inzj through equation (15.64). It is\nnatural to propose the following iterative algorithm.\nDefinition 15.15 (The MSSC algorithm). Let D ={ x1,x2,...,x n}be a finite data set.\nLet Gβ\nα(x,w)be the Gaussian kernel defined as\nGβ\nα(x,w) =e\n−β\nd∑\nh=1\nwα\nhx2\nh\n. (15.70)\nLet v :D →(0,∞) be a weight function defined as\nv(x) = 1\nk∑\nj=1\nGβ\nα(x −zj", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 116, "start": 78880, "end": 79680, "page": 39}
{"text": " as\nGβ\nα(x,w) =e\n−β\nd∑\nh=1\nwα\nhx2\nh\n. (15.70)\nLet v :D →(0,∞) be a weight function defined as\nv(x) = 1\nk∑\nj=1\nGβ\nα(x −zj,wj)\n. (15.71)\nThe sample mean with the kernel Gβ\nα at zj ∈D is defined as\nm(zj) =\n∑\nx∈D\nGβ\nα(x −zj,wj)v(x)x\n∑\nx∈D\nGβ\nα(x −zj,wj)v(x)\n. (15.72)\nLet Z ={ z1,z2,...,z k}⊂D be a finite set of cluster centers. The evolution of Z in the\nform of iterations\nZ(r) =\n{\nz(r)\nj =m(z(r−1)\nj )|j =1,2,...,k\n}\n,r =1,2,..., (15.73)\nis\ncalled the MSSC algorithm, where z(0)\nj = zj,j = 1,2,...,k . The weights v(x) and\nthe fuzzy memberships U = (uji) are reevaluated after each iteration. The sequence\nz,m(z),m2(z),... is called a trajectory of z.\nFrom Definition 9.5, we know that the above fixed-point iteration is the mean shift\nprocess with v(x)being the weight of the pointx. If we choose Z =", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 117, "start": 79560, "end": 80360, "page": 39}
{"text": "that the above fixed-point iteration is the mean shift\nprocess with v(x)being the weight of the pointx. If we choose Z =D, then this algorithm\nis also called a blurring process (Cheng, 1995).\nAlgorithm 15.13. The pseudocode of the MSSC algorithm.\nRequire: D - the data set, k - the number of starting points, α - the fuzzifier, and β - the\nLagrange multiplier;\n1: Initialize Z ={z1,z2,...,z k}by choosing k points from D randomly;\n2: Initialize W ={w1,w2,...,w k}with wjh = 1\nd (1 ≤j ≤k, 1 ≤h ≤d);\n3: Calculate fuzzy memberships U according to equation (15.64) or (15.68);\n4: Znew ⇐m(Z) ={m(z1),m(z 2),...,m( zk)}according to equation (15.63);\n5: Update W according to equation (15.67);\n6: while ∥Znew −Z∥> 0 do\n7: Z ⇐Znew;\n8: Znew ⇐m(Z) according to equation (15.63);\n9: Update W according to equati", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 118, "start": 80240, "end": 81040, "page": 40}
{"text": "5.67);\n6: while ∥Znew −Z∥> 0 do\n7: Z ⇐Znew;\n8: Znew ⇐m(Z) according to equation (15.63);\n9: Update W according to equation (15.67);\n10: Calculate fuzzy memberships U according to Equation (15.64) or (15.68);\n11: end while\n12: Output W, Z, and U.\nLet Z ={ z1,z2,...,z k}be an initial set of centers. Then the procedure defined in\nequation (15.73) determines a set of equilibrium points Z∗ ={z∗\n1,z∗\n2,...,z ∗\nk}for fixed β,\nα, and ε, where z∗\nj = limr→∞mr(zj). In principle, changing k will modify the resulting\nset of equilibrium points. However, there exists some kc such that for all k>k c, one gets\nonly kc distinct limit points for the simple reason that some points converge to the same\nequilibrium point.\nThe pseudocode of the MSSC algorithm is described in Algorithm 15.13. Once\nwe obtain the ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 119, "start": 80920, "end": 81720, "page": 40}
{"text": "to the same\nequilibrium point.\nThe pseudocode of the MSSC algorithm is described in Algorithm 15.13. Once\nwe obtain the fuzzy dimension weight matrix W, the equilibrium set Z, and the fuzzy\nmemberships U, we can use the procedure in Algorithm 15.14 to get the final subspace\nclusters.\nAlgorithm 15.14. The postprocessing procedure to get the final subspace clusters.\nRequire: η, W, Z, and U;\n1: Let Q ={1,2,...,k };\n2: for j =2t ok do\n3: for i =1t oj −1 do\n4: if ∥zj −zi∥∞ <η and i ∈Q then\n5: Q =Q−{j}{zj is identical to zi};\n6: Break;\n7: end if\n8: end for\n9: end for\n10: Let Q ={j1,j2,...,j kc}{kc is the number of distinct equilibrium points};\n11: for i =1t on do\n12: if l0 =arg max\n1≤l≤kc\nujli then\n13: The point xi is assigned to the l0th cluster Cl0 ;\n14: end if\n15: end for\n16: Get set of clust", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 120, "start": 81600, "end": 82400, "page": 40}
{"text": " max\n1≤l≤kc\nujli then\n13: The point xi is assigned to the l0th cluster Cl0 ;\n14: end if\n15: end for\n16: Get set of cluster dimensions Ql for Cl by applying k-means to wjl;\n17: Output Cl and Ql for l =1,2,...,k c.\nIf β = 0, each data point is uniformly associated with all clusters, and thus all the\ncenters z1,z2,...,z k are identical (Gan, 2006). Let kc be the number of distinct centers in\nZ ={z1,z2,...,z k}. Then, at β =0 we have kc =1, but at some positive β we shall have\nkc > 1. In other words, the single cluster will split into smaller clusters. The new clusters\nwill split at higher β.A t β =∞, kc =k, i.e., we shall get k clusters, where k( <n )is the\nnumber of initial centers.\nClearly, when k =1 the objective function is\nFα,β,0(W,Z) =− 1\nβ\nn∑\ni=1\nln\n(\ne−βd1i(W,Z))\n=\nn∑\ni=1\nd1i(W,Z)\n=\nn", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 121, "start": 82280, "end": 83080, "page": 41}
{"text": " centers.\nClearly, when k =1 the objective function is\nFα,β,0(W,Z) =− 1\nβ\nn∑\ni=1\nln\n(\ne−βd1i(W,Z))\n=\nn∑\ni=1\nd1i(W,Z)\n=\nn∑\ni=1\nd∑\nh=1\nwα\n1h(xih −z1h)2,\nwhich has a single minimum or the global minimum; when k ≥2 and β =0 the objective\nfunction is\nFα,0,0(W,Z) =− 1\nβ\nn∑\ni=1\nln(k) =−∞.\nThus, at β = 0 the objective function has a single minimum or the global minimum. At\nhigher β, the objective function may have many local minima.\nSince the objective function has only one minimum whenk =1, to derive the critical\nvalue βα at which the first phase transition occurs, we assume that k> 1 in the following\ndiscussion. We know that at β = βα there is one cluster centered at the mean of the data\nset. Without loss of generality, we move the data set such that the mean of the new data set\nis located at th", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 122, "start": 82960, "end": 83760, "page": 41}
{"text": "f the data\nset. Without loss of generality, we move the data set such that the mean of the new data set\nis located at the origin. Then the first phase transition occurs when the objective function\nhas\nnonzero minimum, i.e., nonzero solution to the equations\n∂Fα,β,0(W,Z)\n∂zjh\n=− 1\nβ\nn∑\ni=1\n2βwα\njh(xih −zjh)e−βdji (W,Z)\nk∑\nl=1\ne−βdli(W,Z)\n=−2wα\njh\nn∑\ni=1\n(xih −zjh)e−βdji (W,Z)\nk∑\nl=1\ne−βdli(W,Z)\n=0, 1 ≤j ≤k, 1 ≤h ≤d,\nand n∑\ni=1\n(xi −zj)e−βdji (W,Z)\nk∑\nl=1\ne−βdli(W,Z)\n=0, 1 ≤j ≤k, (15.74)\nwhere 0 is the zero vector and dji(W,Z) is defined as\ndji(W,Z) =\nd∑\nh=1\nwα\njh(xih −zjh)2, 1 ≤i ≤n, 1 ≤j ≤k.\nNote that in a small neighborhood of the origin, we have\nwjh =wh ≈ 1\nd∑\nl=1\n\n\nn∑\ni=1\nx2\nih\nn∑\ni=1\nx2\nil\n\n\n1\nα−1\n, 1 ≤j ≤k, 1 ≤h ≤d,\nand\nzjh =zh ≈0, 1 ≤j ≤k, 1 ≤h ≤d.\nNow expanding equation (15.74) ", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 123, "start": 83640, "end": 84440, "page": 41}
{"text": "∑\ni=1\nx2\nih\nn∑\ni=1\nx2\nil\n\n\n1\nα−1\n, 1 ≤j ≤k, 1 ≤h ≤d,\nand\nzjh =zh ≈0, 1 ≤j ≤k, 1 ≤h ≤d.\nNow expanding equation (15.74) on a small neighborhood of the origin by Taylor’s series\nin zj =(zj1,zj2,...,z jd )T and ignoring higher-order terms of zjh, we have\n0 =\nn∑\ni=1\n(xi −zj)e−βdji (W,Z)\nk∑\nl=1\ne−βdli(W,Z)\n≈\nn∑\ni=1\n(xi −zj)exp\n(\n−β\nd∑\nt=1\nwα\nt (xit −zjt )2\n)\nkexp\n(\n−β\nd∑\nt=1\nwα\nt x2\nit\n)\n≈ 1\nk\nn∑\ni=1\n(xi −zj)exp\n(\n2β\nd∑\nt=1\nwα\nt xitzjt −β\nd∑\nt=1\nwα\nt z2\njt\n)\n≈ 1\nk\nn∑\ni=1\n(xi −zj)\n(\n1 +2β\nd∑\nt=1\nwα\nt xitzjt\n)\n. (15.75)\nSince ∑n\ni=1 xi = 0, rearranging equation (15.75) and ignoring higher-order terms of zjhs\ngive\nn∑\ni=1\n(\nxi ·2β\nd∑\nt=1\nwα\nt xitzjt −zj\n)\n=0,\nor\nn∑\ni=1\n\n\n2β\n\n\n\nw\nα\n1 x2\ni1 wα\n2 xi1xi2 ··· wα\ndxi1xid\nwα\n1 xi2xi1 wα\n2 x2\ni2 ··· wα\ndxi2xid\n..\n.\n.\n.\n. ... .\n.\n.\nw\nα\n1 xidxi1 w", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 124, "start": 84320, "end": 85120, "page": 42}
{"text": "\n\n\nw\nα\n1 x2\ni1 wα\n2 xi1xi2 ··· wα\ndxi1xid\nwα\n1 xi2xi1 wα\n2 x2\ni2 ··· wα\ndxi2xid\n..\n.\n.\n.\n. ... .\n.\n.\nw\nα\n1 xidxi1 wα\n2 xidxi2 ··· wα\ndx2\nid\n\n\n\n\n\n\nz\nj1\nzj2\n..\n.\nz\njd\n\n\n−\n\n\n\nz\nj1\nzj2\n..\n.\nz\njd\n\n\n\n\n\n=0.\n(15.76)\nLet v\nj =(x1j,x2j,...,x nj) for j =1,2,...,k , and let the variance and covariance\nbe defined as\nvar(vj) = 1\nn\nn∑\ni=1\n(xij −μj)2 = 1\nn\nn∑\ni=1\nx2\nij,\nCor(vj,vl) = 1\nn\nn∑\ni=1\n(xij −μj)(xil −μl),\nwhere\nμj = 1\nn\nn∑\ni=1\nxij, 1 ≤j ≤k.\nFrom equation (15.76), we have\n(I −2βCα)zj =0, (15.77)\nwhere I is the d ×d identity matrix and Cα is the d ×d matrix defined as\nCα =\n\n\n\nw\nα\n1 var(v1)w α\n2 Cor(v1,v2) ··· wα\ndCor(v1,vd)\nwα\n1 Cor(v2,v1)w α\n2 var(v2) ··· wα\ndCor(v2,vd)\n..\n. .\n.\n. ... .\n.\n.\nw\nα\n1 Cor(vd,v1)w α\n2 Cor(vd,v2) ··· wα\ndvar(vd)\n\n\n.\nThus the cr", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 125, "start": 85000, "end": 85800, "page": 43}
{"text": "\n2 var(v2) ··· wα\ndCor(v2,vd)\n..\n. .\n.\n. ... .\n.\n.\nw\nα\n1 Cor(vd,v1)w α\n2 Cor(vd,v2) ··· wα\ndvar(vd)\n\n\n.\nThus the critical value for β is\nβ\nα = 1\n2λmax\n, (15.78)\nwhere λmax is the largest eigenvalue of Cα. Moreover, the center of the new cluster is the\neigenvector of Cα.\n15.13 Summary\nThe subspace clustering introduced in this chapter is an extension of traditional clustering. In\ngeneral, subspace clustering algorithms can be classified into two major categories (Parsons\net\nal., 2004b): top-down algorithms and bottom-up algorithms. A top-down algorithm finds\nan initial clustering in the full set of the dimensions and evaluates the subspaces of each\ncluster, whereas a bottom-up algorithm finds dense regions in low-dimensional spaces and\nthen combines them to form clusters. CLIQUE, ENCLU", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 126, "start": 85680, "end": 86480, "page": 43}
{"text": "bottom-up algorithm finds dense regions in low-dimensional spaces and\nthen combines them to form clusters. CLIQUE, ENCLUS, MAFIA, CLTree, DOC, and CBF\n(Cell-Based Clustering method) (Chang and Jin, 2002) are examples of bottom-up subspace\nclustering algorithms, PART, PROCLUS, ORCLUS, FINDIT, and δ-cluster (Yang et al.,\n2002a) are examples of top-down subspace clustering algorithms. A comparison of these\nsubspace clustering algorithms can be found in Parsons et al. (2004a). Other discussions\nrelated to subspace clustering can be found in Aggarwal and Yu (2002), Amir et al. (2003),\nAgarwal and Mustafa (2004), Domeniconi et al. (2004), Har-Peled and Varadarajan (2002),\nKe and Kanade (2004), Krishnapuram and Freg (1991), Kroeger et al. (2004), Sarafis et al.\n(2003), Wang et al. (2004), and Nar", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 127, "start": 86360, "end": 87160, "page": 44}
{"text": "d Kanade (2004), Krishnapuram and Freg (1991), Kroeger et al. (2004), Sarafis et al.\n(2003), Wang et al. (2004), and Narahashi and Suzuki (2002).", "file": "fd42c24c-1cf5-4fd8-85c5-831d7a1f0c2d__Subspace-Clustering.pdf", "chunk_id": 128, "start": 87040, "end": 87185, "page": 44}
